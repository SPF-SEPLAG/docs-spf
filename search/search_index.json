{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documenta\u00e7\u00e3o Automa\u00e7\u00f5es SPF - SEPLAG","text":"<p>Bem-vindo ao Portal de Refer\u00eancia das Automa\u00e7\u00f5es da Superintend\u00eancia de Planejamento e Finan\u00e7as da Secretaria de Estado de Planejamento e Gest\u00e3o de Minas Gerais, ponto central para toda a documenta\u00e7\u00e3o t\u00e9cnica dos projetos e fluxos automatizados desenvolvidos pela nossa equipe.</p>"},{"location":"#proposito","title":"Prop\u00f3sito","text":"<p>Esta documenta\u00e7\u00e3o tem como objetivo servir como refer\u00eancia t\u00e9cnica e guia de uso para todos os scripts, m\u00f3dulos, rotinas de automa\u00e7\u00e3o e dashboards criados na SPF.  </p> <p>Ela foi projetada para auxiliar desenvolvedores e analistas a entender, manter e evoluir as automatiza\u00e7\u00f5es existentes.</p>"},{"location":"#estrutura","title":"Estrutura","text":"Se\u00e7\u00e3o Descri\u00e7\u00e3o Guias Tutoriais e instru\u00e7\u00f5es passo a passo para utiliza\u00e7\u00e3o dos m\u00f3dulos de automa\u00e7\u00e3o Refer\u00eancia Documenta\u00e7\u00e3o detalhada de classes, fun\u00e7\u00f5es e m\u00f3dulos Fluxos Explica\u00e7\u00f5es sobre a integra\u00e7\u00e3o entre as automatiza\u00e7\u00f5es e os sistemas envolvidos Changelog Registro de atualiza\u00e7\u00f5es, melhorias e novas funcionalidades"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.12</li> <li>Playwright para automa\u00e7\u00e3o de navega\u00e7\u00e3o.</li> <li>Pandas para tratamento e an\u00e1lise de dados.</li> <li>MkDocs para gera\u00e7\u00e3o de documenta\u00e7\u00e3o.</li> <li>Power Automate (Web e Desktop) e Power BI.</li> </ul>"},{"location":"automacoes/filtragem_excel/","title":"Gera\u00e7\u00e3o de dados de pagamento em ordem cronol\u00f3gica","text":"<p>O script python descrito abaixo, de forma geral, realiza a gera\u00e7\u00e3o de dados de pagamento, a partir de de consulta global do B.O (SAP) realizada previamente. </p> <p>O script filtra essa consulta global, gerando dados para o m\u00eas anterior do qual o c\u00f3digo est\u00e1 sendo executado. Esses dados s\u00e3o ent\u00e3o agrupados em tipo de servi\u00e7o e c\u00f3digo da fonte de recurso. </p> <p>Para acessar o reposit\u00f3rio do github, clique aqui. </p> <p>Abaixo segue o detalhamento do script citado.</p>"},{"location":"automacoes/filtragem_excel/#explicacao-do-script-passo-a-passo","title":"Explica\u00e7\u00e3o do script (passo a passo)","text":"<p>Este script carrega um arquivo Excel, adiciona uma coluna calculada chamada \"Tipo\", filtra os dados pelo m\u00eas anterior e, por fim, divide os resultados em v\u00e1rias abas dentro de um novo arquivo Excel \u2014 agrupando por Tipo e Fonte Recurso - C\u00f3digo.</p>"},{"location":"automacoes/filtragem_excel/#passo-a-passo-da-logica","title":"Passo a Passo da L\u00f3gica","text":""},{"location":"automacoes/filtragem_excel/#fluxo-geral","title":"Fluxo geral","text":"flowchart TD A[\"In\u00edcio do Script\"] --&gt; B[\"1. Ler arquivo Excel&lt;br/&gt;pd.read_excel()\"] B --&gt; C[\"2. Limpar colunas&lt;br/&gt;Remover 1\u00aa coluna e cabe\u00e7alhos extras\"] C --&gt; D[\"3. Adicionar coluna 'Tipo'&lt;br/&gt;add_tipo_column()\"] D --&gt; E[\"4. Filtrar pelo m\u00eas anterior&lt;br/&gt;df[df['M\u00eas - Num\u00e9rico'] == past_month]\"] E --&gt; F[\"5. Agrupar por Tipo + Fonte&lt;br/&gt;groupby(['Tipo', 'Fonte Recurso - C\u00f3digo'])\"] F --&gt; G[\"6. Exportar Excel final&lt;br/&gt;Uma aba por grupo\"] G --&gt; H[\"Fim\"]"},{"location":"automacoes/filtragem_excel/#1-importacoes-necessarias","title":"1. Importa\u00e7\u00f5es necess\u00e1rias","text":"<p><pre><code>import pandas as pd\nimport warnings\nfrom datetime import datetime\nfrom filter_data_utils import add_tipo_column\n</code></pre> - pandas \u2192 manipula\u00e7\u00e3o do Excel. - warnings \u2192 usado para esconder warnings do openpyxl. - datetime \u2192 pega o m\u00eas anterior. - add_tipo_column \u2192 fun\u00e7\u00e3o customizada para preencher a coluna Tipo.</p>"},{"location":"automacoes/filtragem_excel/#2-ignorando-warnings-irrelevantes","title":"2. Ignorando warnings irrelevantes","text":"<p><pre><code>warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n</code></pre> Esconde mensagens de alerta (warnings) do openpyxl para n\u00e3o poluir o terminal.</p>"},{"location":"automacoes/filtragem_excel/#3-carregando-o-excel","title":"3. Carregando o Excel","text":"<p><pre><code>file_path = \"./data/dataset.xlsx\"\ndf = pd.read_excel(file_path, sheet_name=\"Relat\u00f3rio 1\", skiprows=3)\ndf = df.iloc[:, 1:]\n</code></pre> - <code>file_path</code> \u2192 define o caminho do diret\u00f3rio do arquivo. - <code>skiprows=3</code> \u2192 pula as 3 primeiras linhas (cabecalhos extras). - <code>df.iloc[:, 1:]</code> \u2192 remove a primeira coluna (vazia ou in\u00fatil).</p>"},{"location":"automacoes/filtragem_excel/#4-criando-a-coluna-tipo","title":"4. Criando a coluna \"Tipo\"","text":"<p><pre><code>df = add_tipo_column(df)\n</code></pre> A fun\u00e7\u00e3o <code>add_tipo_column()</code>: - olha para <code>\"Elemento Item Despesa - C\u00f3digo\"</code> - associa o nome correspondente ao c\u00f3digo, criando nova coluna <code>\"Tipo\"</code> (ex: 3001 \u2192 \"Fornecimento\") - retorna o dataframe com a nova coluna</p>"},{"location":"automacoes/filtragem_excel/#5-filtrando-pelo-mes-anterior","title":"5. Filtrando pelo m\u00eas anterior","text":"<p><pre><code>past_month = datetime.now().month - 1\ndf_filtered = df[df[\"M\u00eas - Num\u00e9rico\"] == past_month]\n</code></pre> - descobre o m\u00eas anterior, - pega apenas as linhas cujo <code>\"M\u00eas - Num\u00e9rico\"</code> coincide.</p>"},{"location":"automacoes/filtragem_excel/#6-agrupando-resultados","title":"6. Agrupando resultados","text":"<p><pre><code>groups = df_filtered.groupby([\"Tipo\", \"Fonte Recurso - C\u00f3digo\"])\n</code></pre> - Aqui voc\u00ea divide os dados como se fossem \u201ccategorias\u201d, por exemplo:</p> Tipo Fonte Recurso - C\u00f3digo Fornecimento 100 Fornecimento 200 Servi\u00e7o 100 <ul> <li>Cada combina\u00e7\u00e3o vira uma aba diferente no Excel final.</li> </ul>"},{"location":"automacoes/filtragem_excel/#7-gerando-o-arquivo-final","title":"7. Gerando o arquivo final","text":"<p><pre><code>output_path = f\"./data/output_{past_month}.xlsx\"\nwith pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n    for k, ((tipo, fonte), group_df) in enumerate(groups):\n        sheet_name = f\"{tipo} - Fonte {fonte}\"\n        group_df.to_excel(writer, sheet_name=sheet_name, index=False)\n</code></pre> Para cada grupo: - cria uma aba com nome <code>\"Tipo - Fonte XYZ\"</code>. - exporta somente as linhas daquele grupo. O resultado final \u00e9 uma pasta de trabalho Excel contendo v\u00e1rias abas separadas por Tipo + Fonte.</p>"},{"location":"dashboards/dccc/intro/","title":"Dashboards PAC, Contratos, Conv\u00eanios e TDCOs","text":"<p>Esses pain\u00e9is foram desenvolvidos para consolidar e monitorar informa\u00e7\u00f5es financeiras referentes ao Planejamento Anual de Compras, Contrata\u00e7\u00f5es, Conv\u00eanios e Termos de Descentraliza\u00e7\u00e3o de Cr\u00e9dito Or\u00e7ament\u00e1rio em que a SEPLAG est\u00e1 envolvida. </p>"},{"location":"dashboards/dccc/intro/#objetivo","title":"Objetivo","text":"<p>O objetivo dos dashboards \u00e9 oferecer vis\u00e3o consolidada e atualizada das principais m\u00e9tricas financeiras relevantes aos gestores de programas e \u00e1reas contratantes, permitindo o acompanhamento da execu\u00e7\u00e3o or\u00e7ament\u00e1ria e a identifica\u00e7\u00e3o de gargalos em tempo real.</p>"},{"location":"dashboards/dccc/intro/#publico-alvo","title":"P\u00fablico-Alvo","text":"<ul> <li>Diretoria de Compras, Contratos e Conv\u00eanios (DCCC).</li> <li>Diretoria de Planejamento e Or\u00e7amento (DPO).</li> <li>Gestores de Programas e Projetos  </li> <li>\u00c1reas contratantes</li> </ul>"},{"location":"dashboards/dccc/intro/#fontes-de-dados","title":"Fontes de Dados","text":"Fonte Tipo Planilhas DCCC Excel (Sharepoint) Bases de dados extra\u00eddas via B.O (BI Corporativo - SAP) Excel (Sharepoint)"},{"location":"dashboards/dccc/intro/#acesso","title":"Acesso","text":"<p>O dashboard est\u00e1 dispon\u00edvel em:</p> <p>Dashboard PAC (Power BI) Dashboard Contratos, Conv\u00eanios e TDCOs(Power BI) </p>"},{"location":"dashboards/dccc/intro/#atualizacoes-e-manutencao","title":"Atualiza\u00e7\u00f5es e Manuten\u00e7\u00e3o","text":"<ul> <li>Os pain\u00e9is s\u00e3o atualizados diariamente, de hora em hora, das 10h-17h, via configura\u00e7\u00e3o do Modelo Sem\u00e2ntico do Power BI online. </li> <li>Al\u00e9m disso, para as bases extra\u00eddas do B.O, \u00e9 necess\u00e1ria uma configura\u00e7\u00e3o adicional para a atualiza\u00e7\u00e3o dos dados por meio de fluxo elaborado a partir de integra\u00e7\u00e3o entre o SAP e o Power Automate Web.</li> </ul>"},{"location":"dashboards/dccc/intro/#tecnologias-e-integracoes","title":"Tecnologias e Integra\u00e7\u00f5es","text":"<ul> <li>Power BI (Desktop e Online) \u2014 plotagem de visualiza\u00e7\u00f5es e atualiza\u00e7\u00e3o autom\u00e1tica dos dados</li> <li>Power Query / DAX \u2014 transforma\u00e7\u00e3o e modelagem de dados  </li> <li>SAP (B.O ou BI Corporativo) - gera\u00e7\u00e3o e envio de bases de dados</li> <li>Power Automate Web \u2014 integra\u00e7\u00e3o de bases de dados</li> </ul>"},{"location":"dashboards/dccc/intro/#contato-responsaveis","title":"Contato / Respons\u00e1veis","text":"<p>O desenvolvimento dos pain\u00e9is foi iniciado na Assessoria da SPF, mas a manuten\u00e7\u00e3o atual est\u00e1 a cargo da Diretoria de Compras, Contratos e Conv\u00eanios.  </p> <p>Para mais informa\u00e7\u00f5es ou suporte t\u00e9cnico, entre em contato com:</p> <ul> <li>Henrique Freitas Dias - Diretor de Compras, Contratos e Conv\u00eanios - henrique.freitas@planejamento.mg.gov.br  </li> <li>Diego Cesar Evangelista Ara\u00fajo - T\u00e9cnico da Diretoria de Compras, Contratos e Conv\u00eanios - diego.araujo@planejamento.mg.gov.br</li> </ul>"},{"location":"dashboards/dpo/1_intro/","title":"Dashboard Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria","text":"<p>Esse painel foi desenvolvido para consolidar informa\u00e7\u00f5es a respeito da Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria da SEPLAG.</p>"},{"location":"dashboards/dpo/1_intro/#objetivo","title":"Objetivo","text":"<p>Oferecer informa\u00e7\u00f5es relevantes e atualizadas, a fim de subsidiar a melhoria da tomada de decis\u00e3o e permitir o melhor acompanhamento da execu\u00e7\u00e3o or\u00e7ament\u00e1ria pelas \u00e1reas t\u00e9cnicas.</p>"},{"location":"dashboards/dpo/1_intro/#publico-alvo","title":"P\u00fablico-alvo","text":"<ul> <li>Diretoria de Planejamento e Or\u00e7amento (DPO).</li> <li>Subsecret\u00e1rios(as) e Secret\u00e1rio(a).</li> <li>\u00c1reas respons\u00e1veis pelas a\u00e7\u00f5es or\u00e7ament\u00e1rias.</li> </ul>"},{"location":"dashboards/dpo/1_intro/#fontes-de-dados","title":"Fontes de Dados","text":"Fonte Tipo Planilha DPO Excel (Sharepoint) Bases de dados tratadas via Python + Github Actions + msal .csv"},{"location":"dashboards/dpo/1_intro/#acesso","title":"Acesso","text":"<p>O dashboard est\u00e1 dispon\u00edvel em:  Dashboard Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria (Power BI) Reposit\u00f3rio github</p>"},{"location":"dashboards/dpo/1_intro/#atualizacoes-e-manutencao","title":"Atualiza\u00e7\u00f5es e Manuten\u00e7\u00e3o","text":"<ul> <li>Os pain\u00e9is s\u00e3o atualizados diariamente, de hora em hora, das 10h-17h.</li> <li>Configura\u00e7\u00e3o do Modelo Sem\u00e2ntico do Power BI online e dos cron jobs do github actions.</li> </ul>"},{"location":"dashboards/dpo/1_intro/#tecnologias-e-integracoes","title":"Tecnologias e integra\u00e7\u00f5es","text":"<ul> <li>Power BI (Desktop e Online) \u2014 plotagem de visualiza\u00e7\u00f5es e atualiza\u00e7\u00e3o autom\u00e1tica dos dados</li> <li>Python (pandas) / Power Query / DAX \u2014 transforma\u00e7\u00e3o e modelagem de dados  </li> <li>github actions \u2014 execu\u00e7\u00e3o autom\u00e1tica de scripts para tratamento e e atualiza\u00e7\u00e3o das bases.</li> </ul>"},{"location":"dashboards/dpo/2_projeto/","title":"Projeto e Cronograma Painel DPO","text":"<p>Este projeto se refere \u00e0 elabora\u00e7\u00e3o do Painel (Dashboard) de Business Intelligence (BI) da Diretoria de Planejamento e Or\u00e7amento (DPO) da Superintend\u00eancia de Planejamento e Finan\u00e7as (SPF) da Subsecretaria de Gest\u00e3o e Finan\u00e7as (SUBGEF) da Secretaria de Planejamento e Gest\u00e3o do Estado de Minas Gerais (SEPLAG-MG).</p> <p>O principal fator identificado durante a investiga\u00e7\u00e3o realizada foi a elevada complexidade t\u00e9cnica associada \u00e0 manuten\u00e7\u00e3o e ao uso da planilha atualmente em vigor. Utilizada h\u00e1 v\u00e1rios anos, essa planilha passou por sucessivos ajustes e adapta\u00e7\u00f5es, o que comprometeu sua estrutura tanto como base de dados quanto como ferramenta de consulta.  </p> <p>Al\u00e9m disso, verificou-se que a forma de organiza\u00e7\u00e3o dos dados dificulta a compreens\u00e3o por parte das \u00e1reas t\u00e9cnicas externas, que enfrentam obst\u00e1culos para acessar as informa\u00e7\u00f5es de maneira clara e visualmente organizada. Isso evidencia uma lacuna entre a forma como os dados est\u00e3o dispostos e sua efetiva utilidade no apoio \u00e0s demandas dessas \u00e1reas.  </p> <p>Os dados presentes na planilha s\u00e3o consumidos e utilizados por agentes de diversos n\u00edveis organizacionais \u2014 t\u00e9cnicos da DPO, \u00e1reas demandantes e tomadores de decis\u00e3o. Portanto, proporcionar uma comunica\u00e7\u00e3o clara e acess\u00edvel \u00e9 fundamental.  </p> <p>Dessa forma, foi identificada a necessidade de melhoria na representa\u00e7\u00e3o e visualiza\u00e7\u00e3o dos dados utilizados no contexto de trabalho da Diretoria, por meio do desenvolvimento de um dashboard de Business Intelligence.</p>"},{"location":"dashboards/dpo/2_projeto/#objetivos","title":"Objetivos","text":"<p>Este projeto possui os seguintes objetivos:</p> <ul> <li>Subsidiar o desenvolvimento de um dashboard de BI para apoiar processos de trabalho e tomada de decis\u00e3o.  </li> <li>Documentar o processo de trabalho realizado pelos t\u00e9cnicos desta Assessoria, servindo como metodologia base para futuros pain\u00e9is ou automa\u00e7\u00f5es.  </li> <li>Apoiar a elabora\u00e7\u00e3o de um cronograma com prazos, metas e entregas previstas.</li> </ul> <p>O resultado esperado \u00e9 a entrega de um painel validado pelos usu\u00e1rios e em conformidade com as demandas operacionais da DPO, das \u00e1reas externas e dos tomadores de decis\u00e3o.</p>"},{"location":"dashboards/dpo/2_projeto/#metodologia","title":"Metodologia","text":"<p>Para a defini\u00e7\u00e3o do escopo do projeto, foram realizadas reuni\u00f5es com a DPO e estudo da planilha Excel utilizada como fonte de dados.  </p> <p>O painel ser\u00e1 estruturado para contemplar duas abas principais:</p> <ul> <li>Informa\u00e7\u00f5es T\u00e9cnicas: voltadas ao contexto operacional da DPO, para acompanhamento di\u00e1rio.  </li> <li>Informa\u00e7\u00f5es Gerenciais: voltadas \u00e0 avalia\u00e7\u00e3o estrat\u00e9gica e tomada de decis\u00e3o dos gestores.  </li> </ul> <p>A metodologia de desenvolvimento adotada foi incremental, com entregas evolutivas em fases e ciclos, atendendo \u00e0s seguintes particularidades:</p> <ul> <li>Entregas parciais e r\u00e1pidas  </li> <li>Engajamento cont\u00ednuo dos envolvidos  </li> <li>Redu\u00e7\u00e3o de retrabalho  </li> </ul> <p>Ferramentas utilizadas: - Power BI, Power Query, Excel Online, SharePoint - Python (<code>msal</code>, <code>Office365-REST-Python-Client</code>, <code>pandas</code>) - GitHub, GitHub Actions - Git Bash (recomendado) - VS Code (recomendado)</p>"},{"location":"dashboards/dpo/2_projeto/#fases-e-ciclos","title":"Fases e Ciclos","text":"<p>Fase 0 \u2014 Alinhamento e Planejamento Inicial</p> <ul> <li>Conversas iniciais com a DPO (mapeamento do processo, entendimento do problema)  </li> <li>Estudo inicial da fonte de dados  </li> <li>Defini\u00e7\u00e3o das fases do projeto e cronograma  </li> </ul> <p>Ao final, foi decidido dividir o desenvolvimento do painel em tr\u00eas fases: 1. Aba t\u00e9cnica (informa\u00e7\u00f5es para \u00e1reas e t\u00e9cnicos da DPO) 2. Aba gerencial (informa\u00e7\u00f5es para gestores) 3. Testes e homologa\u00e7\u00e3o final  </p> <p>Fase 1 e Fase 2 \u2014 Desenvolvimento das Abas</p> <p>Etapas por ciclo:</p> <ol> <li>Conversas com a DPO \u2014 defini\u00e7\u00e3o de escopo, layout e prioridades.  </li> <li>Modelagem de Dados e ETL \u2014 extra\u00e7\u00e3o, transforma\u00e7\u00e3o e padroniza\u00e7\u00e3o.  </li> <li>Desenvolvimento do Painel \u2014 constru\u00e7\u00e3o de indicadores e visualiza\u00e7\u00f5es.  </li> <li>Valida\u00e7\u00e3o \u2014 testes de consist\u00eancia e coleta de feedbacks.  </li> </ol> <p>Fase 3 \u2014 Valida\u00e7\u00e3o e Homologa\u00e7\u00e3o Final</p> <p>Essa fase garante estabilidade t\u00e9cnica, confiabilidade dos dados e boa usabilidade. Recomenda-se realizar um teste piloto com usu\u00e1rios-chave de diferentes perfis (analistas, gestores, operacionais).</p> <p>Etapas: - Coleta de feedback sobre clareza, utilidade e erros. - Ajustes e homologa\u00e7\u00e3o final antes da libera\u00e7\u00e3o.  </p>"},{"location":"dashboards/dpo/2_projeto/#cronograma","title":"Cronograma","text":"Fase Atividade Respons\u00e1veis Prazo Estimado Fase 0 Reuni\u00f5es iniciais com a DPO Assessoria SPF + DPO 2 dias Fase 0 Estudo da fonte de dados Assessoria SPF 4 dias Fase 0 Defini\u00e7\u00e3o de fases e cronograma Assessoria SPF 3 dias Fase 0 Valida\u00e7\u00e3o do planejamento Assessoria SPF + DPO 1 dia Fase 1 Conversas com DPO (escopo, layout) Assessoria SPF + DPO 1 dia (por ciclo) Fase 1 Modelagem de dados e ETL Assessoria SPF 3 dias (por ciclo) Fase 1 Desenvolvimento do painel Assessoria SPF 2 dias (por ciclo) Fase 1 Valida\u00e7\u00e3o com t\u00e9cnicos DPO + Assessoria SPF 1 dia (por ciclo) Fase 2 Conversas com gestores Assessoria SPF + DPO 1 dia (por ciclo) Fase 2 Modelagem de dados gerencial Assessoria SPF 3 dias (por ciclo) Fase 2 Desenvolvimento dos pain\u00e9is gerenciais Assessoria SPF 2 dias (por ciclo) Fase 2 Valida\u00e7\u00e3o com gestores DPO + Assessoria SPF 1 dia (por ciclo) Fase 3 Prepara\u00e7\u00e3o para piloto Assessoria SPF 1 dia Fase 3 Teste piloto com a \u00e1rea Assessoria SPF + DPO 5 dias Fase 3 Coleta e an\u00e1lise de feedbacks Assessoria SPF 1 dia Fase 3 Ajustes finais Assessoria SPF 3 dias Fase 3 Homologa\u00e7\u00e3o e entrega DPO + Assessoria SPF 1 dia <p>Tempo total estimado</p> <ul> <li>Fase inicial: 10 dias \u00fateis </li> <li>Desenvolvimento (3 ciclos por aba): 42 dias \u00fateis </li> <li>Fase final de testes: 10 dias \u00fateis Total: 62 dias \u00fateis</li> </ul>"},{"location":"dashboards/dpo/2_projeto/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>O cronograma pode sofrer ajustes conforme surgirem situa\u00e7\u00f5es imprevistas ou limita\u00e7\u00f5es t\u00e9cnicas. Por exemplo, nas fases iniciais, foi necess\u00e1rio utilizar Python e GitHub Actions para o tratamento automatizado dos dados devido ao alto volume e limita\u00e7\u00e3o do Power BI/hardware.</p> <p>Essa rotina Python automatizada ser\u00e1 documentada em uma se\u00e7\u00e3o pr\u00f3pria, dada sua complexidade e import\u00e2ncia para o funcionamento do painel.</p>"},{"location":"dashboards/dpo/3_fluxo_geral/","title":"Integra\u00e7\u00e3o das ferramentas e fluxo geral","text":"<p>Para garantir o processamento e a atualiza\u00e7\u00e3o dos dados em tempo real, foi necess\u00e1rio definir um fluxo de ETL (Extract, Transform, Load) que englobasse todo o processo: download autom\u00e1tico da planilha, tratamento e modelagem dos dados via <code>pandas</code>, e carregamento dos resultados no painel do Power BI \u2014 tudo de forma totalmente automatizada.  </p> <p>Para isso, foram investigadas quais ferramentas melhor atenderiam a cada etapa e como integr\u00e1-las em um \u00fanico fluxo cont\u00ednuo. Em linhas gerais, s\u00e3o utilizadas as seguintes ferramentas, de forma integrada: Sharepoint, python, Github e PBI.</p> <p>Abaixo est\u00e1 uma explica\u00e7\u00e3o resumida de cada tecnologia envolvida e do fluxo geral da rotina. </p>"},{"location":"dashboards/dpo/3_fluxo_geral/#git-e-github","title":"Git e GitHub","text":"<p>Para saber o que \u00e9 o Git e o GitHub, leia aqui.</p> <p>O GitHub permite armazenar e versionar c\u00f3digo na nuvem, al\u00e9m de integrar ferramentas para automa\u00e7\u00e3o, como o GitHub Actions. O projeto desenvolvido utiliza o GitHub como reposit\u00f3rio central e controle de versionamento via Git.</p>"},{"location":"dashboards/dpo/3_fluxo_geral/#github-actions","title":"GitHub Actions","text":"<p>Para execu\u00e7\u00e3o autom\u00e1tica dos scripts foi utilizado o GitHub Actions.</p> <p>Para saber o que \u00e9 o GitHub Actions, leia aqui.</p> <p>As VMs disponibilizadas pelo GitHub Actions permitem a automa\u00e7\u00e3o completa dos scripts de download e tratamento de dados utilizados pela DPO. Ap\u00f3s o processamento, os arquivos <code>.csv</code> gerados s\u00e3o automaticamente commitados de volta ao reposit\u00f3rio, prontos para o carregamento no Power BI.</p>"},{"location":"dashboards/dpo/3_fluxo_geral/#secrets-e-personal-access-token-pat","title":"Secrets e Personal Access Token (PAT)","text":"<p>Por motivos de seguran\u00e7a, o reposit\u00f3rio GitHub foi configurado como privado, permitindo acesso apenas a usu\u00e1rios autorizados.</p> <p>Para que a rotina automatizada funcione corretamente \u2014 especialmente nos momentos em que a VM precisa enviar commits ao reposit\u00f3rio e carregar dados no Power BI \u2014 \u00e9 necess\u00e1rio o uso de credenciais de acesso.  </p> <p>Essas credenciais s\u00e3o configuradas por meio dos Secrets e Personal Access Token (PAT), definidos dentro do pr\u00f3prio GitHub. O passo a passo para cria\u00e7\u00e3o dessas credenciais est\u00e1 descrito neste guia.</p>"},{"location":"dashboards/dpo/3_fluxo_geral/#msal-e-office365-rest-python-client","title":"msal e Office365-REST-Python-Client","text":""},{"location":"dashboards/dpo/3_fluxo_geral/#download-automatico-da-planilha-do-sharepoint","title":"Download autom\u00e1tico da planilha do SharePoint","text":"<p>Uma das etapas mais importantes \u00e9 o download automatizado da planilha hospedada no SharePoint, utilizando Python.  </p> <p>Para isso, foram usadas as bibliotecas: <code>msal</code> e <code>Office365-REST-Python-Client</code>.</p> <p>Ambas s\u00e3o mantidas pela Microsoft e permitem a integra\u00e7\u00e3o com os servi\u00e7os do Office 365 via API, utilizando credenciais corporativas.</p> <p>Com elas, foi poss\u00edvel construir um script Python capaz de baixar automaticamente a planilha diretamente do SharePoint.</p>"},{"location":"dashboards/dpo/3_fluxo_geral/#criacao-do-access-token-e-refresh-token","title":"Cria\u00e7\u00e3o do Access Token e Refresh Token","text":"<p>Durante a elabora\u00e7\u00e3o do script de download, identificou-se um desafio: a necessidade de login manual com a conta organizacional. Isso impediria a execu\u00e7\u00e3o automatizada do fluxo completo.</p> <p>Para resolver esse problema, foi implementada uma autentica\u00e7\u00e3o autom\u00e1tica com o SharePoint, utilizando a gera\u00e7\u00e3o de Access Tokens e Refresh Tokens via <code>msal</code>.</p> <p>Para entender o que s\u00e3o Access Token e Refresh Token, leia aqui.</p> <p>Na primeira execu\u00e7\u00e3o do script, o login \u00e9 feito manualmente, e os tokens gerados s\u00e3o armazenados em um arquivo de cache (<code>msal_cache.bin</code>).  Esse arquivo \u00e9 ent\u00e3o commitado no reposit\u00f3rio privado do GitHub.</p> <p>Nas execu\u00e7\u00f5es seguintes, o fluxo usa o Refresh Token armazenado para gerar automaticamente um novo Access Token, sem necessidade de login manual. Assim, o GitHub Actions executa todo o processo de autentica\u00e7\u00e3o e download sem interven\u00e7\u00e3o humana.</p> <p>O Refresh Token tamb\u00e9m possui tempo de expira\u00e7\u00e3o. Ainda n\u00e3o h\u00e1 uma defini\u00e7\u00e3o exata sobre sua validade, portanto \u00e9 necess\u00e1rio monitorar as execu\u00e7\u00f5es dos workflows do GitHub Actions e regerar manualmente os tokens quando necess\u00e1rio.</p>"},{"location":"dashboards/dpo/3_fluxo_geral/#obtencao-dos-scripts-de-download-credenciais-organizacionais-api-sharepoint-e-geracoes-dos-tokens","title":"Obten\u00e7\u00e3o dos scripts de download, credenciais organizacionais API sharepoint e gera\u00e7\u00f5es dos Tokens","text":"<p>Deve-se ressaltar que a elabora\u00e7\u00e3o da rotina descrita aqui contou com suporte da equipe da Assessoria da Intelig\u00eancia de Dados da Subsecretaria Central de Planejamento e Or\u00e7amento da SEPLAG/MG. Al\u00e9m do aux\u00edlio para projetar e implementar a rotina, foi cedido o script sharepoint_utils.py que cont\u00e9m as fun\u00e7\u00f5es de gerar os Tokens e de realizar download e upload de arquivos do Sharepoint, al\u00e9m das credenciais organizacionais de acesso aos servi\u00e7os do Office 365 via <code>API</code>. </p>"},{"location":"dashboards/dpo/3_fluxo_geral/#fluxo-geral","title":"Fluxo geral","text":"flowchart TD A[\"In\u00edcio do processo\"] --&gt; B[\"Execu\u00e7\u00e3o manual inicial do script Pythonde download da planilha da DPO\"] B --&gt; C[\"Login manual na conta organizacional (MSAL)\"] C --&gt; D[\"Gera\u00e7\u00e3o do Access Token e Refresh Token\"] D --&gt; E[\"Armazenamento em cache local (msal_cache.bin)\"] E --&gt; F[\"Commit e push manual do cache para o reposit\u00f3rio privado do GitHub\"] F --&gt; G[\"Execu\u00e7\u00e3o autom\u00e1tica da rotina via GitHub Actions\"] G --&gt; H[\"Download automatizado da planilha do SharePoint\"] H --&gt; I[\"Tratamento e modelagem dos dados com pandas\"] I --&gt; J[\"Exporta\u00e7\u00e3o dos CSVs tratados\"] J --&gt; K[\"Commit autom\u00e1tico dos CSVs no reposit\u00f3rio via workflow.yml\"] K --&gt; L[\"Carregamento dos dados no Power BI\"] L --&gt; M[\"Fim do processo\"]"},{"location":"dashboards/dpo/3_fluxo_geral/#comentarios-sobre-o-fluxo","title":"Coment\u00e1rios sobre o fluxo","text":"<ul> <li>A primeira execu\u00e7\u00e3o requer login, commit e push manualmente para gerar os tokens iniciais e o arquivo cache.</li> <li>As execu\u00e7\u00f5es seguintes s\u00e3o 100% autom\u00e1ticas, at\u00e9 o vencimento do Refresh Token.  </li> <li>O GitHub Actions atua como o motor de automa\u00e7\u00e3o, orquestrando todo o fluxo ETL.</li> </ul>"},{"location":"dashboards/dpo/4_download/","title":"Download da planilha do Sharepoint e gera\u00e7\u00e3o dos Tokens","text":"<p>Para que a rotina funcione automaticamente via GitHub Actions, \u00e9 necess\u00e1rio gerar o Access Token e o Refresh Token uma \u00fanica vez, manualmente. Esses tokens ficam salvos em um arquivo de cache e ser\u00e3o reutilizados nas execu\u00e7\u00f5es seguintes.</p> <p>As pr\u00f3ximas execu\u00e7\u00f5es ser\u00e3o autom\u00e1ticas e silenciosas, sem necessidade de login manual.</p>"},{"location":"dashboards/dpo/4_download/#1-visao-geral-do-processo","title":"1. Vis\u00e3o Geral do Processo","text":"<ul> <li>Na primeira execu\u00e7\u00e3o, o usu\u00e1rio realiza login interativo no navegador.</li> <li>O script salva os tokens no arquivo <code>msal_cache.bin</code>.</li> <li>Nas execu\u00e7\u00f5es seguintes, o script utiliza o Refresh Token para obter o Access Token silenciosamente.</li> <li>O token obtido permite requisitar a API do SharePoint e realizar o download da planilha.</li> </ul>"},{"location":"dashboards/dpo/4_download/#2-arquivo-principal-downloadpy","title":"2. Arquivo principal: <code>download.py</code>","text":"<pre><code>from sharepoint_utils import download_sharepoint_file, get_sharepoint_token\n\nbase_url = \"https://cecad365.sharepoint.com/sites/DPO\"\nfolder_path = \"/sites/DPO/Documentos Compartilhados/1 Or\u00e7amento/1 Acompanhamento da Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria/Execu\u00e7\u00e3o 2025\"\n\ndownload_sharepoint_file(base_url, folder_path, \"Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx\", \"Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx\")\n</code></pre>"},{"location":"dashboards/dpo/4_download/#3-funcao-de-download","title":"3. Fun\u00e7\u00e3o de Download","text":"<pre><code>def download_sharepoint_file(base_url, folder_path, file_name, local_filename):\n    \"\"\"\n    Downloads a file from SharePoint using dynamic components for the URL.\n    \"\"\"\n</code></pre> <p>Par\u00e2metros: - <code>base_url</code> \u2014 URL base do site SharePoint. - <code>folder_path</code> \u2014 caminho interno da pasta onde o arquivo est\u00e1. - <code>file_name</code> \u2014 nome do arquivo no SharePoint. - <code>local_filename</code> \u2014 nome com que ser\u00e1 salvo localmente.</p> <ul> <li>Antes do download, a fun\u00e7\u00e3o chama <code>get_sharepoint_token()</code>, respons\u00e1vel por obter o Access Token.</li> </ul>"},{"location":"dashboards/dpo/4_download/#4-configuracoes-de-autenticacao","title":"4. Configura\u00e7\u00f5es de Autentica\u00e7\u00e3o","text":"<p><pre><code>config = {\n  \"authority\": \"https://login.microsoftonline.com/e5d3ae7c-9b38-48de-a087-f6734a287574\",\n  \"client_id\": \"d44a05d5-c6a5-4bbb-82d2-443123722380\",\n  \"scope\": [\"https://cecad365.sharepoint.com/.default\"], #[\"Group.ReadWrite.All\"],\n  \"username\": \"username@ca.mg.gov.br\",\n  \"endpoint\": \"https://login.microsoftonline.com/common/oauth2/v2.0/authorize\"\n}\n</code></pre> - Somente o campo <code>username</code> deve ser alterado - Trocar: <code>username@ca.mg.gov.br</code> \u2192 por um login CA v\u00e1lido do respons\u00e1vel pela rotina.</p>"},{"location":"dashboards/dpo/4_download/#5-geracao-dos-tokens-get_sharepoint_token","title":"5. Gera\u00e7\u00e3o dos Tokens: get_sharepoint_token()","text":""},{"location":"dashboards/dpo/4_download/#51-carregamento-do-cache","title":"5.1. Carregamento do Cache","text":"<p><pre><code>def get_sharepoint_token():\n    # \u2705 Load or create token cache\n    cache = msal.SerializableTokenCache()\n    if os.path.exists(CACHE_FILE):\n        cache.deserialize(open(CACHE_FILE, \"r\").read())\n</code></pre> - Se existir um cache, ele \u00e9 carregado, junto com o Access Token e o Refresh Token. - Na primeira execu\u00e7\u00e3o, o cache estar\u00e1 vazio.</p>"},{"location":"dashboards/dpo/4_download/#52-inicializacao-do-aplicativo-msal","title":"5.2. Inicializa\u00e7\u00e3o do aplicativo MSAL","text":"<p><pre><code>    app = msal.PublicClientApplication(\n        config[\"client_id\"], authority=config[\"authority\"], token_cache=cache\n        )\n</code></pre> O aplicativo \u00e9 carregado com: - credenciais da organiza\u00e7\u00e3o, - tokens existentes no cache (se houver).</p>"},{"location":"dashboards/dpo/4_download/#53-verificacao-de-contas-ja-autenticadas","title":"5.3. Verifica\u00e7\u00e3o de contas j\u00e1 autenticadas","text":"<p><pre><code>    accounts = app.get_accounts(username=config.get(\"username\"))\n</code></pre> - Verifica se j\u00e1 existe token v\u00e1lido para o usu\u00e1rio informado. - Na primeira execu\u00e7\u00e3o: estar\u00e1 vazio.</p>"},{"location":"dashboards/dpo/4_download/#54-aquisicao-silenciosa-do-access-token","title":"5.4. Aquisi\u00e7\u00e3o silenciosa do Access Token","text":"<p><pre><code>    if accounts:\n        result = app.acquire_token_silent(config[\"scope\"], account=accounts[0])\n</code></pre> Se o cache cont\u00e9m um Refresh Token v\u00e1lido: - O Access Token \u00e9 obtido silenciosamente. - \u00c9 esse processo que permite a automa\u00e7\u00e3o via GitHub Actions.</p>"},{"location":"dashboards/dpo/4_download/#55-aquisicao-interativa-primeira-execucao","title":"5.5. Aquisi\u00e7\u00e3o interativa (primeira execu\u00e7\u00e3o)","text":"<p><pre><code>    if not result:\n        # So no suitable token exists in cache. Let's get a new one from Azure AD.\n        result = app.acquire_token_interactive(scopes=config[\"scope\"])\n</code></pre> Caso n\u00e3o exista token v\u00e1lido: - Abre o navegador. - Usu\u00e1rio faz login com a conta organizacional. - Novos tokens s\u00e3o gerados.</p>"},{"location":"dashboards/dpo/4_download/#56-salvamento-do-cache","title":"5.6. Salvamento do Cache","text":"<p><pre><code>    # \u2705 Save updated cache\n    if cache.has_state_changed:\n        with open(CACHE_FILE, \"w\") as f:\n            f.write(cache.serialize())\n        print(\"\ud83d\udcbe Token cache saved.\")\n</code></pre> - Tokens novos ou atualizados s\u00e3o gravados no arquivo <code>msal_cache.bin</code>. - Se o arquivo n\u00e3o existir, ele ser\u00e1 criado.</p>"},{"location":"dashboards/dpo/4_download/#6-conclusao","title":"6. Conclus\u00e3o","text":"<p>A fun\u00e7\u00e3o <code>get_sharepoint_token()</code> retorna o Access Token obtido, seja: - silenciosamente via Refresh Token, ou - manualmente na primeira execu\u00e7\u00e3o. Esse token \u00e9 ent\u00e3o usado pela fun\u00e7\u00e3o <code>download_sharepoint_file()</code> para baixar a planilha do SharePoint.</p>"},{"location":"dashboards/dpo/4_download/#7-comentarios-finais","title":"7. Coment\u00e1rios Finais","text":"<ul> <li>A vers\u00e3o original da fun\u00e7\u00e3o de autentica\u00e7\u00e3o fornecida pela equipe da SPLOR estava com falhas e foi redesenhada pela equipe de automa\u00e7\u00e3o da SPF.</li> <li>O foco deste documento est\u00e1 na parte mais sens\u00edvel e complexa do fluxo: a gera\u00e7\u00e3o e gerenciamento dos Tokens.</li> <li>Para detalhes da implementa\u00e7\u00e3o do download em si, consultar o script original.</li> </ul>"},{"location":"dashboards/dpo/5_pre_processamento_dados/","title":"Pr\u00e9-processamento dos dados","text":"<p>Como detalhado nas se\u00e7\u00f5es anteriores, devido \u00e0 complexidade da planilha utilizada no dia-a-dia de trabalho da DPO e por limita\u00e7\u00f5es do PBI ao processar o grande volume de dados foi necess\u00e1rio realizar o tratamento inicial dos dados utilizando outra solu\u00e7\u00e3o que n\u00e3o fosse o Power Query ou o DAX. </p> <p>Para isso, foi feito um pr\u00e9-processamento por meio do python, utilizando a lib do <code>pandas</code>, amplamente utilizada para leitura, limpeza, transforma\u00e7\u00e3o e an\u00e1lise de dados.  </p>"},{"location":"dashboards/dpo/5_pre_processamento_dados/#acesso","title":"Acesso","text":"<ul> <li>Para acessar os scripts, acesse o repo pelo github. <p>Caso n\u00e3o consiga visualizar o repo, pe\u00e7a para o respons\u00e1vel adicion\u00e1-lo como colaborador.</p> </li> </ul>"},{"location":"dashboards/dpo/5_pre_processamento_dados/#pre-processamento-da-planilha-da-dpo-process_datacsv","title":"Pr\u00e9-processamento da planilha da DPO (process_data.csv)","text":""},{"location":"dashboards/dpo/5_pre_processamento_dados/#etapa-1-filtrar-abas-relevantes","title":"Etapa 1 - Filtrar abas relevantes","text":"<ul> <li>A fun\u00e7\u00e3o <code>get_tables()</code> identifica quais abas da planilha cont\u00eam dados v\u00e1lidos.</li> <li>Ela ignora abas desnecess\u00e1rias como \u201cMenu\u201d e mant\u00e9m apenas aquelas com 4 caracteres.</li> </ul> <p><pre><code>def get_tables(excel_file):\n  # Get all sheet names\n  xls = pd.ExcelFile(excel_file)\n  sheet_names = xls.sheet_names\n  filtered_sheets = []\n  # Filter sheet names:\n  # - Keep only 4-character names\n  # - Remove 'Menu'\n  # - Remove names starting with '44'\n  for sheet in sheet_names:\n      if len(sheet) == 4 and sheet not in ['Menu', '44xx', '4488', '4489']:\n          filtered_sheets.append(sheet)\n\n  return filtered_sheets\n</code></pre> - Sa\u00edda: uma lista de nomes de abas filtradas.</p>"},{"location":"dashboards/dpo/5_pre_processamento_dados/#etapa-2-processar-abas-textuais","title":"Etapa 2 - Processar abas textuais","text":"<ul> <li>A fun\u00e7\u00e3o <code>process_tables_text()</code> trata colunas de texto, como \u201cObjeto\u201d e \u201cPrograma\u00e7\u00e3o\u201d. Principais a\u00e7\u00f5es</li> <li>Encontra a linha onde come\u00e7a a tabela (procura \u201cPrograma\u00e7\u00e3o\u201d).</li> <li>Remove linhas e colunas extras.</li> <li>\"Limpa\" a coluna \"Objeto\" para evitar erros de atualiza\u00e7\u00e3o por conta do preenchimento dos dados da planilha pelos t\u00e9cnicos (padroniza os textos -&gt; tudo em min\u00fasculo, tira acentos e pontua\u00e7\u00e3o, etc).</li> <li>Remove linhas baseado em valores espec\u00edficos de GMIFP.</li> </ul> <p><pre><code>def process_tables_text(excel_file, filtered_sheets):\n  treated_dfs = []\n  removed_ids = []\n  df = pd.DataFrame()\n\n  for sheet in filtered_sheets:\n      df = pd.read_excel(excel_file, sheet_name=sheet)\n      # Find the row index where first column contains 'Programa\u00e7\u00e3o'\n      target_row = df[df.iloc[:, 0].str.contains('Programa\u00e7\u00e3o', case=False, na=False)].index[0]\n      # Get all data starting from the target row\n      df = df.iloc[target_row:]\n      # Drop rows with empty values in the first column\n      df = df.dropna(subset=[df.columns[0]])\n      # Drop last row\n      df = df[:-1]\n      # Keep only the first 9 columns\n      df = df.iloc[:, :9]\n      # Promote first row to headers\n      df.columns = df.iloc[0]\n      df = df.iloc[1:]  # Remove the first row since it's now the header\n      # Add sheet name as 'A\u00e7\u00e3o'\n      df['A\u00e7\u00e3o'] = sheet\n\n      # Cleans column Objeto\n      df[\"Objeto\"] = (\n          df[\"Objeto\"]\n          .fillna(\"\")\n          .str.strip()\n          .str.lower()\n          .apply(lambda x: re.sub(r'[^\\w\\s]', \" \", x, flags=re.UNICODE))\n          .str.split()\n          .str.join(\" \")\n          .str.title()\n      )\n\n      #GMIFPs to remove\n      gmis_to_remove = [\"1.90.0.10.1\", \"1.91.0.10.1\", \"3.90.0.10.7\"]\n\n      # Get which rows will be removed\n      rows_to_remove = df[df['GMIFP'].isin(gmis_to_remove)]\n\n      # Extract the IDs of those rows for later processing by process_tables_num\n      removed_ids.extend(rows_to_remove['Programa\u00e7\u00e3o'].tolist())\n\n      # Effectively remove rows from df by GMIFP\n      df = df[~df['GMIFP'].isin(gmis_to_remove)]\n\n      print(\"Tratando aba:\", sheet)\n      treated_dfs.append(df)\n\n  if treated_dfs:\n    final_df = pd.concat(treated_dfs, ignore_index=True)\n    output_file = 'processed_data_text.csv'\n    final_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n    print(f\"\\nData exported to {output_file}\")\n\n    return removed_ids\n</code></pre> - Sa\u00edda: exporta os dados tratados para <code>processed_data_text.csv</code> e retorna uma lista de IDs das linhas removidas.</p>"},{"location":"dashboards/dpo/5_pre_processamento_dados/#etapa-3-processar-abas-numericas","title":"Etapa 3 \u2014 Processar abas num\u00e9ricas","text":"<ul> <li>A fun\u00e7\u00e3o <code>process_tables_num()</code> trata os valores financeiros.</li> <li>Ela usa os IDs removidos na etapa anterior para excluir registros indesejados. Principais a\u00e7\u00f5es</li> <li>Localiza a linha \u201cPrograma\u00e7\u00e3o\u201d e ajusta o in\u00edcio da tabela</li> <li>Remove colunas desnecess\u00e1rias (como \u201cTOTAIS\u201d)</li> <li>Transp\u00f5e os dados (linhas \u2194 colunas) e em seguida faz o unpivot e pivot para reorganizar os valores. <p>O passo 3 \u00e9 imprescind\u00edvel para associar corretamente o n\u00famero de programa\u00e7\u00e3o aos meses e \u00e0s respectivas colunas de valores, quais sejam \"Descentralizado\", \"Empenhado\", \"Liquidado\", \"Programado\" e \"Realizado\".</p> </li> <li>Converte colunas para n\u00fameros.</li> <li>Remove as linhas com os respectivos IDs do passo anterior (associados aos GMIFPs que devem ser removidos).</li> </ul> <p><pre><code>def process_tables_num(excel_file, filtered_sheets, removed_ids):\n  treated_dfs = []\n\n  for worksheet in filtered_sheets:\n      df = pd.read_excel(excel_file, sheet_name=worksheet)\n\n      # 1. Find the row index where first column contains 'programa\u00e7\u00e3o'\n      target_row = df[df.iloc[:, 0].str.contains('Programa\u00e7\u00e3o', case=False, na=False)].index[0]\n\n      # 2. Filter the DataFrame to start 2 rows before the target row\n      df = df.iloc[target_row-2:]\n\n      # 3. Remove rows where first column is null, but keep the first row\n      first_row = df.iloc[0:1]  # Keep first row\n      rest_of_df = df.iloc[1:].dropna(subset=[df.columns[0]])  # Remove nulls from rest\n      df = pd.concat([first_row, rest_of_df])  # Combine them back\n\n      # 4. Remove the last row\n      df = df.iloc[:-1]\n\n      # 5. Find the column with 'TOTAIS' in the second row and remove it and all columns after it\n      total_col_idx = df.iloc[1].str.contains('TOTAIS', case=False, na=False).idxmax()\n      total_col_num = df.columns.get_loc(total_col_idx)\n      df = df.iloc[:, :total_col_num]  # Keep all columns up to (but not including) the TOTAIS column\n\n      # 6. Remove columns 2 through 9\n      df = pd.concat([df.iloc[:, 0:1], df.iloc[:, 9:]], axis=1)\n\n      # 7. Transpose the DataFrame\n      df = df.transpose()\n\n      # 8. Remove the second column\n      df = df.drop(df.columns[1], axis=1)\n\n      # 9. Promote first row to headers\n      df.columns = df.iloc[0]  # Set column names to first row\n      df = df.iloc[1:]  # Remove the first row since it's now the header\n\n      # 10. Rename the first two columns\n      df = df.rename(columns={df.columns[0]: 'M\u00eas', df.columns[1]: 'Status'})\n\n      # 11. Unpivot the DataFrame\n      id_vars = ['M\u00eas', 'Status']  # Columns to keep as identifiers\n      df = pd.melt(df, \n                  id_vars=id_vars,\n                  var_name='Programa\u00e7\u00e3o',  # Name for the column containing the former column headers\n                  value_name='Valor')    # Name for the column containing the values\n\n      # 12. Add A\u00e7\u00e3o column with worksheet name\n      df['A\u00e7\u00e3o'] = worksheet\n\n      # 13. Pivot the table back using Status and Valor\n      df = df.pivot(index=['M\u00eas', 'Programa\u00e7\u00e3o', 'A\u00e7\u00e3o'], \n                  columns='Status', \n                  values='Valor').reset_index()\n\n      # 14. Force columns to be number types (filter unexpected characters in number columns)\n      columns_to_clean = ['Descentralizado', 'Empenhado', 'Liquidado', \"Programado\", \"Realizado\"]\n\n      for col in columns_to_clean:\n          # Remove tudo que n\u00e3o \u00e9 n\u00famero e converte para float, ou NaN se falhar\n          df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n\n      # 15. Remove rows where gmifp == [\"1.90.0.10.1\", \"1.91.0.10.1\", \"3.90.0.10.7\"] by n. Programa\u00e7\u00e3o\n      df = df[~df['Programa\u00e7\u00e3o'].isin(removed_ids)]\n\n      # Add to list of DataFrames\n      treated_dfs.append(df)\n      print(f\"Processed worksheet: {worksheet}\")\n\n  # Combine all DataFrames\n  if treated_dfs:\n      final_df = pd.concat(treated_dfs, ignore_index=True)\n\n      # Export to CSV\n      output_file = 'processed_data_num.csv'\n      final_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n      print(f\"\\nData exported to {output_file}\")\n</code></pre> - Sa\u00edda: exporta o resultado para <code>processed_data_num.csv</code>.</p>"},{"location":"dashboards/dpo/5_pre_processamento_dados/#execucao-do-script","title":"Execu\u00e7\u00e3o do Script","text":"<ul> <li>Por fim, basta passar o nome da planilha e chamar as fun\u00e7\u00f5es na sequ\u00eancia:</li> </ul> <pre><code>excel_file = 'Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx'\n\nfiltered_sheets = get_tables(excel_file)\nremoved_ids = process_tables_text(excel_file, filtered_sheets)\nprocess_tables_num(excel_file, filtered_sheets, removed_ids)\n</code></pre>"},{"location":"dashboards/dpo/5_pre_processamento_dados/#resultado-esperado","title":"Resultado esperado","text":"<ul> <li><code>processed_data_text.csv</code></li> <li><code>processed_data_num.csv</code></li> </ul> <p>Esses arquivos ser\u00e3o integrados via GitHub Actions e ent\u00e3o usados pelo Power BI.</p>"},{"location":"dashboards/dpo/6_workflow_github/","title":"Documenta\u00e7\u00e3o do Workflow","text":"<p>Ap\u00f3s a implementa\u00e7\u00e3o dos scripts de download da planilha, gera\u00e7\u00e3o dos Tokens e processamento dos dados, \u00e9 necess\u00e1rio a configura\u00e7\u00e3o do ambiente no github. </p> <p>Ap\u00f3s a cria\u00e7\u00e3o do reposit\u00f3rio privado, e gera\u00e7\u00e3o do PAT e dos Secrets, resta a elabora\u00e7\u00e3o do script <code>.yml</code> que ir\u00e1 coordenar a execu\u00e7\u00e3o do Github Actions.</p> <p>Este workflow do GitHub Actions \u00e9 respons\u00e1vel por executar automaticamente os scripts Python detalhados anteriormente em hor\u00e1rios espec\u00edficos do dia, al\u00e9m de permitir execu\u00e7\u00f5es manuais. O objetivo principal \u00e9:</p> <ul> <li>Baixar a planilha do SharePoint (<code>download.py</code>)</li> <li>Processar os dados e gerar arquivos CSV (<code>process_data.py</code>)</li> <li>Fazer commit autom\u00e1tico no reposit\u00f3rio dos arquivos gerados</li> </ul>"},{"location":"dashboards/dpo/6_workflow_github/#1-agendamento-das-execucoes","title":"1. Agendamento das Execu\u00e7\u00f5es","text":"<p>O workflow utiliza cron jobs para rodar o processo de hora em hora, sempre no minuto 30, respeitando o hor\u00e1rio de Bras\u00edlia (UTC\u22123).</p>"},{"location":"dashboards/dpo/6_workflow_github/#horarios-de-execucao","title":"Hor\u00e1rios de Execu\u00e7\u00e3o","text":"<p>Cada cron job listado abaixo representa um hor\u00e1rio convertido para UTC:</p> Hor\u00e1rio BRT Cron (UTC) 09:30 <code>30 12 * * *</code> 10:30 <code>30 13 * * *</code> 11:30 <code>30 14 * * *</code> 12:30 <code>30 15 * * *</code> 13:30 <code>30 16 * * *</code> 14:30 <code>30 17 * * *</code> 15:30 <code>30 18 * * *</code> <p>Al\u00e9m disso, existe o gatilho:</p> <pre><code>workflow_dispatch:\n</code></pre> <p>que permite executar o workflow manualmente pelo GitHub.</p>"},{"location":"dashboards/dpo/6_workflow_github/#2-permissoes","title":"2. Permiss\u00f5es","text":"<pre><code>permissions:\n  contents: write\n</code></pre> <p>Isso permite que o workflow fa\u00e7a commits autom\u00e1ticos no reposit\u00f3rio.</p>"},{"location":"dashboards/dpo/6_workflow_github/#3-job-principal","title":"3. Job Principal","text":"<pre><code>jobs:\n  run-script:\n    runs-on: ubuntu-latest\n</code></pre> <p>O processo roda em uma m\u00e1quina virtual Ubuntu fornecida pelo GitHub.</p>"},{"location":"dashboards/dpo/6_workflow_github/#31-checkout-do-repositorio","title":"3.1. Checkout do reposit\u00f3rio","text":"<p><pre><code>      - name: Checkout repository\n        uses: actions/checkout@v3\n</code></pre> Baixa o c\u00f3digo do reposit\u00f3rio para a m\u00e1quina virtual.</p>"},{"location":"dashboards/dpo/6_workflow_github/#32-configuracao-do-python","title":"3.2. Configura\u00e7\u00e3o do Python","text":"<p><pre><code>      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.12'\n</code></pre> Instala e ativa o Python 3.12.</p>"},{"location":"dashboards/dpo/6_workflow_github/#33-instalacao-das-dependencias","title":"3.3. Instala\u00e7\u00e3o das depend\u00eancias","text":"<p><pre><code>      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n</code></pre> Atualiza o pip e instala depend\u00eancias se existir um arquivo <code>requirements.txt</code>.</p>"},{"location":"dashboards/dpo/6_workflow_github/#34-execucao-do-script-de-download","title":"3.4. Execu\u00e7\u00e3o do script de download","text":"<p><pre><code>      - name: Run Python script\n        run: python download.py\n</code></pre> Executa a l\u00f3gica que: - acessa o SharePoint via MSAL - baixa a planilha - salva no reposit\u00f3rio</p>"},{"location":"dashboards/dpo/6_workflow_github/#35-processamento-dos-dados","title":"3.5. Processamento dos dados","text":"<p><pre><code>      - name: Generate CSV Number and Text File\n        run: python process_data.py   # \u2b05\ufe0f Your custom CSV logic\n</code></pre> Esse script realiza: - Transforma\u00e7\u00f5es com Pandas - Gera\u00e7\u00e3o de CSVs com os dados </p>"},{"location":"dashboards/dpo/6_workflow_github/#36-commit-e-push-automatico","title":"3.6. Commit e push autom\u00e1tico","text":"<pre><code>      - name: Commit and push downloaded file\n        env:\n          TOKEN: ${{ secrets.PERSONAL_TOKEN }}\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"github-actions[bot]@users.noreply.github.com\"\n          git add .\n          git commit -m \"Add downloaded file\" || echo \"No changes to commit\"\n          git remote set-url origin https://x-access-token:${TOKEN}@github.com/${{ github.repository }}\n          git push origin HEAD:${{ github.ref_name }}\n</code></pre> <p>Este passo:</p> <ul> <li>configura a identidade do Git  </li> <li>adiciona os novos arquivos  </li> <li>cria um commit (se houver mudan\u00e7as)  </li> <li>envia ao reposit\u00f3rio usando um token pessoal</li> </ul> <p>**PAREI AQUI*** <pre><code>git commit -m \"Add downloaded file\" || echo \"No changes to commit\"\n</code></pre></p> <p>Caso n\u00e3o existam altera\u00e7\u00f5es, o commit \u00e9 ignorado.</p> <p>\u26a0 Seguran\u00e7a: O token usado (<code>${{ secrets.PERSONAL_TOKEN }}</code>) deve ter permiss\u00f5es de repo.</p>"},{"location":"dashboards/dpo/6_workflow_github/#4-visao-geral-do-fluxo","title":"\ud83e\uddea 4. Vis\u00e3o Geral do Fluxo","text":"<ol> <li>Workflow dispara automaticamente ou manualmente  </li> <li>Python \u00e9 configurado  </li> <li>Scripts s\u00e3o executados  </li> <li>Arquivos gerados s\u00e3o commitados no reposit\u00f3rio  </li> <li>Workflow finaliza</li> </ol>"},{"location":"dashboards/dpo/6_workflow_github/#sugestao-de-titulo-para-o-arquivo-de-documentacao","title":"\ud83d\udcdd Sugest\u00e3o de t\u00edtulo para o arquivo de documenta\u00e7\u00e3o","text":"<ul> <li><code>workflow_execucao_automatica.md</code></li> <li><code>automacao_download_sharepoint.md</code></li> <li><code>pipeline_processamento_sharepoint.md</code></li> </ul>"},{"location":"dashboards/dpo/detalhamentos/","title":"Download da planilha do Sharepoint e gera\u00e7\u00e3o dos Tokens","text":"<p>Como citado anteriormente, a primeira execu\u00e7\u00e3o do script dever\u00e1 ser feita de forma manual, para gera\u00e7\u00e3o do Access e Refresh Tokens, salvos no arquivo de cache.</p> <p>As execu\u00e7\u00f5es seguintes s\u00e3o feitas de forma automatizada pelo Github Actions. </p> <p>download.py <pre><code>from sharepoint_utils import download_sharepoint_file, get_sharepoint_token\n\nbase_url = \"https://cecad365.sharepoint.com/sites/DPO\"\nfolder_path = \"/sites/DPO/Documentos Compartilhados/1 Or\u00e7amento/1 Acompanhamento da Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria/Execu\u00e7\u00e3o 2025\"\n\ndownload_sharepoint_file(base_url, folder_path, \"Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx\", \"Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx\")\n</code></pre></p> <p>sharepoint_utils.py <pre><code>def download_sharepoint_file(base_url, folder_path, file_name, local_filename):\n    \"\"\"\n    Downloads a file from SharePoint using dynamic components for the URL.\n\n    Args:\n        base_url (str): The base SharePoint site URL (e.g., 'https://cecad365.sharepoint.com/sites/Splor').\n        folder_path (str): The relative path to the folder containing the file (e.g., '/Documentos Compartilhados/General').\n        file_name (str): The name of the file to download (e.g., 'datamart.xlsx').\n        local_filename (str): The local path where the file will be saved.\n\n    Returns:\n        bool: True if the file was downloaded successfully, False otherwise.\n    \"\"\"\n</code></pre></p> <ul> <li> <p>Como par\u00e2metros da fun\u00e7\u00e3o s\u00e3o chamados a URL do site da organiza\u00e7\u00e3o no Sharepoint e o caminho da pasta em que o arquivo est\u00e1 salvo. Em seguida, de acordo com as anota\u00e7\u00f5es da fun\u00e7\u00e3o, s\u00e3o passados o nome do arquivo que ser\u00e1 baixado e o caminho local em que o arquivo ser\u00e1 baixado.</p> </li> <li> <p>Na execu\u00e7\u00e3o da fun\u00e7\u00e3o de download, primeiro \u00e9 chamada a fun\u00e7\u00e3o de gera\u00e7\u00e3o dos Tokens. \u00c9 a fun\u00e7\u00e3o <code>get_sharepoint_token()</code>.</p> </li> <li>Essa fun\u00e7\u00e3o utiliza a vari\u00e1vel <code>config</code>, que cont\u00e9m os dados necess\u00e1rios para validar o acesso e realizar o credenciamento para utiliza\u00e7\u00e3o da <code>API</code> do Office 365.</li> </ul> <p>sharepoint_utils.py <pre><code>config = {\n  \"authority\": \"https://login.microsoftonline.com/e5d3ae7c-9b38-48de-a087-f6734a287574\",\n  \"client_id\": \"d44a05d5-c6a5-4bbb-82d2-443123722380\",\n  \"scope\": [\"https://cecad365.sharepoint.com/.default\"], #[\"Group.ReadWrite.All\"],\n  \"username\": \"username@ca.mg.gov.br\",\n  \"endpoint\": \"https://login.microsoftonline.com/common/oauth2/v2.0/authorize\"\n}\n</code></pre> - A \u00fanica informa\u00e7\u00e3o que dever\u00e1 ser alterada aqui \u00e9 a vinculada \u00e0 chave de username, com os dados da conta organizacional do usu\u00e1rio respons\u00e1vel pela execu\u00e7\u00e3o da rotina . </p> <p>Mudar <code>username@ca.mg.gov.br</code> para login ca v\u00e1lido. </p> <p>sharepoint_utils.py <pre><code>def get_sharepoint_token():\n    # \u2705 Load or create token cache\n    cache = msal.SerializableTokenCache()\n    if os.path.exists(CACHE_FILE):\n        cache.deserialize(open(CACHE_FILE, \"r\").read())\n</code></pre> - Inicialmente tenta-se ler o arquivo cache em que estar\u00e3o armazenadas as informa\u00e7\u00f5es de acesso do usu\u00e1rio e os Tokens de credenciamento, o Access Token e o Refresh Token. Caso esse arquivo exista, ele \u00e9 lido e armazenado na vari\u00e1vel cache.</p> <p><pre><code>    app = msal.PublicClientApplication(\n        config[\"client_id\"], authority=config[\"authority\"], token_cache=cache\n        )\n</code></pre> - O app \u00e9 ent\u00e3o carregado com, al\u00e9m das credenciais da organiza\u00e7\u00e3o, as informa\u00e7\u00f5es presentes no cache, se existirem.</p> <p><pre><code>    # initialize result variable to hole the token response\n    result = None \n</code></pre> - A vari\u00e1vel <code>result</code> armazenar\u00e1 a resposta da aquisi\u00e7\u00e3o do Access Token, silenciosamente ou iterativamente (detalhado abaixo).</p> <p><pre><code>    # We now check the cache to see\n    # whether we already have some accounts that the end user already used to sign in before.\n    accounts = app.get_accounts(username=config.get(\"username\"))\n    print(accounts)\n</code></pre> - A pr\u00f3xima etapa \u00e9 checar se a credencial do usu\u00e1rio corresponde a algum dos usu\u00e1rios listados no cache. Ou seja, se o usu\u00e1rio, conforme inserido na vari\u00e1vel username \u00e9 autenticado e possui algum Token correspondente. Caso seja, a vari\u00e1vel account ser\u00e1 carregada com os dados desse usu\u00e1rio e caso n\u00e3o, ela ser\u00e1 vazia. \u00c9 v\u00e1lido ressaltar que na primeira execu\u00e7\u00e3o do script, essa vari\u00e1vel accounts ser\u00e1 vazia.</p> <p><pre><code>    if accounts:\n        result = app.acquire_token_silent(config[\"scope\"], account=accounts[0])\n</code></pre> - Caso seja encontrado os dados daquele usu\u00e1rio no cache, o usu\u00e1rio \u00e9 autenticado, ou seja, o Access Token \u00e9 adquirido silenciosamente a partir do Refresh Token.  - \u00c9 essa aquisi\u00e7\u00e3o silenciosa do Access Token que permite as execu\u00e7\u00f5es automatizadas da rotina pelo Github Actions.</p> <p><pre><code>    if not result:\n        # So no suitable token exists in cache. Let's get a new one from Azure AD.\n        result = app.acquire_token_interactive(scopes=config[\"scope\"])\n</code></pre> - Em caso de ainda n\u00e3o haver tokens nem cache gerados, ou seja, em se tratando de primeira execu\u00e7\u00e3o, os Tokens dever\u00e3o ser gerado manualmente. - A fun\u00e7\u00e3o <code>acquire_token_interactive()</code> abre o navegador e pede para o usu\u00e1rio logar na sua conta organizacional e, assim, se credenciar interativamente, gerando um novo Access Token e Refresh Token.</p> <p><pre><code>    # \u2705 Save updated cache\n    if cache.has_state_changed:\n        with open(CACHE_FILE, \"w\") as f:\n            f.write(cache.serialize())\n        print(\"\ud83d\udcbe Token cache saved.\")\n</code></pre> - O cache de acesso, junto dos novos Tokens quando gerados s\u00e3o ent\u00e3o salvos no arquivo <code>msal_cache.bin</code> (se ele n\u00e3o existia previamente na pasta do projeto ele \u00e9 criado aqui). </p> <ul> <li>De um jeito ou de outro ser\u00e1 gerado o Access Token (ou interativamente ou silenciosamente, via Refresh Token), que \u00e9 o que permite a conex\u00e3o com a <code>API</code> do Sharepoint. </li> <li>Como dito previamente, a vari\u00e1vel <code>result</code> armazenar\u00e1 a resposta da aquisi\u00e7\u00e3o desse Token. Quando a aquisi\u00e7\u00e3o ocorrer corretamente, a fun\u00e7\u00e3o <code>get_sharepoint_token()</code> ir\u00e1 retornar essa vari\u00e1vel, que ser\u00e1 utilizada em sequ\u00eancia pela fun\u00e7\u00e3o <code>download_sharepoint_file()</code> para realizar o download da planilha que ser\u00e1 tratada.</li> </ul> <p>Coment\u00e1rios - A fun\u00e7\u00e3o originalmente utilizada pela equipe da Assessoria da Intelig\u00eancia de Dados da SPLOR para a gera\u00e7\u00e3o dos Tokens n\u00e3o estava funcionando corretamente e teve que ser redesenhada. A fun\u00e7\u00e3o acima foi a vers\u00e3o final desenvolvida pela equipe de automa\u00e7\u00e3o de processos da SPF.  - N\u00e3o ser\u00e1 detalhado aqui o resto da funcionalidade da fun\u00e7\u00e3o de download, por motivos de n\u00e3o se achar necess\u00e1rio faz\u00ea-lo para o restante da compreens\u00e3o do fluxo e por conta da complexidade do processo estar concentrado na maior parte na gera\u00e7\u00e3o dos Tokens.  - Para maior detalhamento da fun\u00e7\u00e3o de download, consultar o script original.</p>"},{"location":"guides/camg/config_proxy/","title":"Configurar proxy prodemge nas vari\u00e1veis de ambiente","text":"<p>Casos em que \u00e9 necess\u00e1rio: bloqueio github, requisi\u00e7\u00f5es bloquedas, etc.</p> <p></p>"},{"location":"guides/camg/config_proxy/#windows","title":"Windows","text":"<p>Editar vari\u00e1veis de ambiente &gt; Nova vari\u00e1vel de sistema</p> <p>http_proxy http://usuario:senha@proxyint.prodemge.gov.br:8080 https_proxy http://usuario:senha@proxyint.prodemge.gov.br:8080</p>"},{"location":"guides/camg/config_proxy/#linux-wsl","title":"Linux (wsl)","text":"<p>Adicionar ao final do arquivo <code>.bashrc</code> na raiz do wsl:</p> <pre><code>export HTTP_PROXY=\"http://usuario:senha@proxyint.prodemge.gov.br:8080\"\nexport HTTPS_PROXY=\"http://usuario:senha@proxyint.prodemge.gov.br:8080\"\nexport NO_PROXY=\"localhost,127.0.0.1,::1\"\n\nexport http_proxy=\"http://usuario:senha@proxyint.prodemge.gov.br:8080\"\nexport https_proxy=\"http://usuario:senha@proxyint.prodemge.gov.br:8080\"\nexport no_proxy=\"localhost,127.0.0.1,::1\"\n</code></pre> <ul> <li>Usu\u00e1rio \u00e9 o login ca (<code>m753270</code>, por exemplo).</li> <li>A senha tem que ser encapsulada em formato encoding URL. </li> <li>Se o usu\u00e1rio/senha do usu\u00e1rio logado n\u00e3o funcionar, pode ser que tentar usar o login do Administrador de rede funcione.</li> </ul>"},{"location":"guides/camg/config_ssh/","title":"Configurar uso de chaves SSH na CAMG","text":"<p>\u00c9 preciso mudar a porta utilizada quando tentando usar chaves SSH na CAMG.</p>"},{"location":"guides/camg/config_ssh/#linux-wsl","title":"Linux (wsl)","text":"<ul> <li>Na raiz do wsl, na pasta <code>.ssh</code>, onde estar\u00e3o armazenadas as chaves SSH, criar arquivo <code>config</code> (\u00e9 s\u00f3 <code>config</code>, n\u00e3o <code>config.txt</code>).</li> </ul> <pre><code>Host github.com\n  HostName ssh.github.com\n  Port 443\n  User git\n</code></pre>"},{"location":"guides/github/criar_repo/","title":"Criar reposit\u00f3rio github","text":"<p> - N\u00e3o \u00e9 necess\u00e1rio marcar nenhuma das outras op\u00e7\u00f5es. Apenar defina o nome do reposit\u00f3rio e marque-o como privado ou p\u00fablico.  </p>"},{"location":"guides/github/criar_secrets_pat/","title":"Setup Secrets, PAT e Actions Github","text":"<p>\u00c9 necess\u00e1rio criar o Secret e o Personal Access Token (PAT) no Github para correta execu\u00e7\u00e3o dos Actions do Github quando o reposit\u00f3rio for privado. </p>"},{"location":"guides/github/criar_secrets_pat/#criar-pat","title":"Criar PAT","text":"<p>Conta &gt; Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic) &gt; Generate new token &gt; Generate new token (classic) &gt; Note: gh-action-push &gt; Select scopes: repo (marcar checkbox) &gt; Generate token &gt; Copiar o hash do token </p> <p>Sugest\u00e3o: copiar e salvar para um txt, pois voc\u00ea n\u00e3o conseguir\u00e1 ver ele novamente. </p> <p> </p>"},{"location":"guides/github/criar_secrets_pat/#criar-secret","title":"Criar Secret","text":"<p>Reposit\u00f3rio &gt; Settings &gt; Secrets and variables &gt; Actions &gt; New repository secret &gt; Name: PERSONAL_TOKEN, Secret: colar a hash do PAT que voc\u00ea copiou &gt; Add secret </p> <p> </p>"},{"location":"guides/github/criar_ssh/","title":"Criar ssh","text":"<p>AINDA EM ELABORA\u00c7\u00c3O</p>"},{"location":"guides/github/conceitos/access_refresh_tokens/","title":"Access Token e Refresh Token","text":""},{"location":"guides/github/conceitos/access_refresh_tokens/#o-que-sao-access-token-e-refresh-token","title":"O que s\u00e3o Access Token e Refresh Token?","text":"<p>Segundo Auth0, quando um usu\u00e1rio faz login, o servidor de autoriza\u00e7\u00e3o emite um token de acesso (Access Token), que \u00e9 um artefato que as aplica\u00e7\u00f5es cliente podem usar para fazer chamadas seguras para um servidor de API. Quando uma aplica\u00e7\u00e3o cliente precisa acessar recursos protegidos em um servidor em nome de um usu\u00e1rio, o token de acesso permite que o cliente sinalize ao servidor que recebeu autoriza\u00e7\u00e3o do usu\u00e1rio para executar determinadas tarefas ou acessar determinados recursos. </p> <p>\u00c9 fundamental ter estrat\u00e9gias de seguran\u00e7a que minimizem o risco de comprometer os tokens de acesso. Um m\u00e9todo de mitiga\u00e7\u00e3o \u00e9 criar tokens de acesso com vida \u00fatil curta: eles s\u00e3o v\u00e1lidos apenas por um curto per\u00edodo definido em termos de horas ou dias. </p> <p>Depois que expiram, as aplica\u00e7\u00f5es cliente podem usar um Refresh Token para \"atualizar\" o token de acesso. Ou seja, um Refresh Token \u00e9 um artefato de credencial que permite que uma aplica\u00e7\u00e3o cliente obtenha novos tokens de acesso sem precisar solicitar que o usu\u00e1rio fa\u00e7a login novamente. </p> <p>Dessa forma, devido ao tempo curto de expira\u00e7\u00e3o do Access Token, em rotinas de automa\u00e7\u00e3o que requerem credenciamento de usu\u00e1rios, como \u00e9 o caso do processamento dos dados no painel de Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria, o Refresh Token \u00e9 utilizado para gerar novos Access Tokens continuamente e validar a execu\u00e7\u00e3o da rotina junto \u00e0 API espec\u00edfica que est\u00e1 sendo utilizada.</p>"},{"location":"guides/github/conceitos/github_actions/","title":"O que \u00e9 o Github Actions?","text":"<p>Segundo a documenta\u00e7\u00e3o oficial do Github, \u201co gitHub fornece m\u00e1quinas virtuais do Linux, Windows e macOS para executar seus fluxos de trabalho, ou voc\u00ea pode hospedar seus pr\u00f3prios executores auto-hospedados na sua pr\u00f3pria infraestrutura de dados ou na nuvem.\u201d </p> <p>A partir da configura\u00e7\u00e3o de um arquivo de script .yml \u00e9 poss\u00edvel estipular a execu\u00e7\u00e3o desses fluxos de trabalho pelas m\u00e1quinas virtuais disponibilizadas pelo github, de forma autom\u00e1tica, definindo inclusive agendamentos de hor\u00e1rios para execu\u00e7\u00e3o (chamados de cron jobs). </p> <p>A partir das VMs disponibilizadas pelo Github Actions e pela defini\u00e7\u00e3o dos cron jobs por meio do arquivo <code>.yml</code> se torna poss\u00edvel automa\u00e7\u00e3o de tarefas atrav\u00e9s da execu\u00e7\u00e3o autom\u00e1tica de scripts. </p>"},{"location":"guides/pbi/carregar_dados/","title":"Carregar dados PBI","text":""},{"location":"guides/pbi/carregar_dados/#carregamento-de-dados-de-planilha-do-sharepoint-no-power-bi","title":"Carregamento de dados de Planilha do Sharepoint no Power BI","text":"<ul> <li> <p>Obter dados no Power BI  </p> </li> <li> <p>Extrair links da planilha no Sharepoint  </p> </li> <li> <p>Colar link no Power BI </p> </li> </ul>"},{"location":"guides/pbi/carregar_dados/#carregamento-de-dados-do-github-no-power-bi","title":"Carregamento de dados do Github no Power BI","text":"<ul> <li> <p>Extrair links das bases  </p> </li> <li> <p>Criar par\u00e2metro no PBI e copiar e colar a hash do Personal Access Token (PAT) do Github, criado em momento pr\u00e9vio </p> </li> <li> <p>Criar consulta nula que ir\u00e1 armazenar os dados</p> </li> </ul> <p></p> <ul> <li>Carregar dados via Power Query M, copiando e colando o c\u00f3digo abaixo no PBI, alterando a URL</li> </ul> <p></p> <p>O c\u00f3digo citado serve para consumir os dados via API do pr\u00f3prio github, sendo passado no cabe\u00e7alho da requisi\u00e7\u00e3o o tipo Bearer e o PAT.</p> <pre><code>url = \"https://api.github.com/repos/SPF-SEPLAG/painel-dpo/contents/processed_data_num.csv\",\nauthHeader = \"Bearer \" &amp; GitHubToken,\nresponse = Json.Document(Web.Contents(url, [\n    Headers = [Authorization = authHeader]\n])),\nbase64 = response[content],\nbinary = Binary.FromText(base64, BinaryEncoding.Base64),\ncsv = Csv.Document(binary, [Delimiter = \",\", Encoding = 65001, QuoteStyle = QuoteStyle.Csv]),\ntable = Table.PromoteHeaders(csv),\n</code></pre>"},{"location":"guides/pbi/configurar_modelo_semantico/","title":"Configurar Modelo Sem\u00e2ntico PBI","text":"<ul> <li>Uma vez elaborado o relat\u00f3rio, ser\u00e1 necess\u00e1rio realizar a publica\u00e7\u00e3o dos dados </li> </ul>"},{"location":"guides/pbi/configurar_modelo_semantico/#gerar-o-link-do-dashboard","title":"Gerar o link do dashboard","text":"<ul> <li>Acesse o relat\u00f3rio do painel no PBI Web </li> </ul>"},{"location":"guides/pbi/configurar_modelo_semantico/#acessar-o-modelo-semantico","title":"Acessar o modelo sem\u00e2ntico","text":""},{"location":"guides/pbi/configurar_modelo_semantico/#configurar-credenciais-bases-github","title":"Configurar credenciais bases Github","text":"<ul> <li>Definir as credenciais de acesso para as bases do github  </li> </ul>"},{"location":"guides/pbi/configurar_modelo_semantico/#configurar-credenciais-bases-excel-sharepoint","title":"Configurar credenciais bases Excel Sharepoint","text":""},{"location":"guides/pbi/configurar_modelo_semantico/#definir-periodicidade-de-atualizacao-dos-paineis","title":"Definir periodicidade de atualiza\u00e7\u00e3o dos pain\u00e9is","text":"<ul> <li>\u00c9 poss\u00edvel definir a frequ\u00eancia de atualiza\u00e7\u00e3o e inserir contato de e-mail que ser\u00e1 notificado em caso de erro de atualiza\u00e7\u00e3o  </li> </ul> <p>Tamb\u00e9m \u00e9 poss\u00edvel realizar a atualiza\u00e7\u00e3o manualmente caso necess\u00e1rio  </p>"},{"location":"guides/sap/rotina_bo/","title":"Rotina bo","text":"<p>Documentar - fluxos automa\u00e7\u00f5es</p> <ul> <li>Este documento tem o intuito de documentar a rotina de atualiza\u00e7\u00e3o de dados de painel BI do BO institucional via Power Automate. </li> <li>Introdu\u00e7\u00e3o sobre Painel DPO</li> <li>Processo de Tratamento de dados Python Painel DPO  </li> <li> <p>Elaborar p\u00e1gina introdut\u00f3ria na se\u00e7\u00e3o overview dashboards direcionando os links</p> </li> <li> <p>Explicar l\u00f3gica de constru\u00e7\u00e3o dos pain\u00e9is</p> </li> <li> <p>Salvar arquivos dos paineis</p> </li> <li>CLonar repos github</li> <li>Salvar arquivos das automa\u00e7\u00f5es</li> </ul>"}]}