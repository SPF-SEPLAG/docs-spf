{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documenta\u00e7\u00e3o Automa\u00e7\u00f5es SPF - SEPLAG","text":"<p>Bem-vindo ao Portal de Refer\u00eancia das Automa\u00e7\u00f5es da Superintend\u00eancia de Planejamento e Finan\u00e7as da Secretaria de Estado de Planejamento e Gest\u00e3o de Minas Gerais, ponto central para toda a documenta\u00e7\u00e3o t\u00e9cnica dos projetos e fluxos automatizados desenvolvidos pela nossa equipe.</p>"},{"location":"#proposito","title":"Prop\u00f3sito","text":"<p>Esta documenta\u00e7\u00e3o tem como objetivo servir como refer\u00eancia t\u00e9cnica e guia de uso para todos os scripts, m\u00f3dulos, rotinas de automa\u00e7\u00e3o e dashboards criados na SPF.  </p> <p>Ela foi projetada para auxiliar desenvolvedores e analistas a entender, manter e evoluir as automatiza\u00e7\u00f5es existentes.</p>"},{"location":"#estrutura","title":"Estrutura","text":"Se\u00e7\u00e3o Descri\u00e7\u00e3o Guias Tutoriais e instru\u00e7\u00f5es passo a passo para utiliza\u00e7\u00e3o dos m\u00f3dulos de automa\u00e7\u00e3o Refer\u00eancia Documenta\u00e7\u00e3o detalhada de classes, fun\u00e7\u00f5es e m\u00f3dulos Fluxos Explica\u00e7\u00f5es sobre a integra\u00e7\u00e3o entre as automatiza\u00e7\u00f5es e os sistemas envolvidos Changelog Registro de atualiza\u00e7\u00f5es, melhorias e novas funcionalidades"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.12</li> <li>Playwright para automa\u00e7\u00e3o de navega\u00e7\u00e3o.</li> <li>Pandas para tratamento e an\u00e1lise de dados.</li> <li>MkDocs para gera\u00e7\u00e3o de documenta\u00e7\u00e3o.</li> <li>Power Automate (Web e Desktop) e Power BI.</li> </ul>"},{"location":"automacoes/filtragem_excel/","title":"Gera\u00e7\u00e3o de dados de pagamento em ordem cronol\u00f3gica","text":"<p>O script python descrito abaixo, de forma geral, realiza a gera\u00e7\u00e3o de dados de pagamento, a partir de de consulta global do B.O (SAP) realizada previamente. </p> <p>O script filtra essa consulta global, gerando dados para o m\u00eas anterior do qual o c\u00f3digo est\u00e1 sendo executado. Esses dados s\u00e3o ent\u00e3o agrupados em tipo de servi\u00e7o e c\u00f3digo da fonte de recurso. </p> <p>Para acessar o reposit\u00f3rio do github, clique aqui. </p> <p>Abaixo segue o detalhamento do script citado.</p>"},{"location":"automacoes/filtragem_excel/#explicacao-do-script-passo-a-passo","title":"Explica\u00e7\u00e3o do script (passo a passo)","text":"<p>Este script carrega um arquivo Excel, adiciona uma coluna calculada chamada \"Tipo\", filtra os dados pelo m\u00eas anterior e, por fim, divide os resultados em v\u00e1rias abas dentro de um novo arquivo Excel \u2014 agrupando por Tipo e Fonte Recurso - C\u00f3digo.</p>"},{"location":"automacoes/filtragem_excel/#passo-a-passo-da-logica","title":"Passo a Passo da L\u00f3gica","text":""},{"location":"automacoes/filtragem_excel/#fluxo-geral","title":"Fluxo geral","text":"flowchart TD A[\"In\u00edcio do Script\"] --&gt; B[\"1. Ler arquivo Excel&lt;br/&gt;pd.read_excel()\"] B --&gt; C[\"2. Limpar colunas&lt;br/&gt;Remover 1\u00aa coluna e cabe\u00e7alhos extras\"] C --&gt; D[\"3. Adicionar coluna 'Tipo'&lt;br/&gt;add_tipo_column()\"] D --&gt; E[\"4. Filtrar pelo m\u00eas anterior&lt;br/&gt;df[df['M\u00eas - Num\u00e9rico'] == past_month]\"] E --&gt; F[\"5. Agrupar por Tipo + Fonte&lt;br/&gt;groupby(['Tipo', 'Fonte Recurso - C\u00f3digo'])\"] F --&gt; G[\"6. Exportar Excel final&lt;br/&gt;Uma aba por grupo\"] G --&gt; H[\"Fim\"]"},{"location":"automacoes/filtragem_excel/#1-importacoes-necessarias","title":"1. Importa\u00e7\u00f5es necess\u00e1rias","text":"<p><pre><code>import pandas as pd\nimport warnings\nfrom datetime import datetime\nfrom filter_data_utils import add_tipo_column\n</code></pre> - pandas \u2192 manipula\u00e7\u00e3o do Excel. - warnings \u2192 usado para esconder warnings do openpyxl. - datetime \u2192 pega o m\u00eas anterior. - add_tipo_column \u2192 fun\u00e7\u00e3o customizada para preencher a coluna Tipo.</p>"},{"location":"automacoes/filtragem_excel/#2-ignorando-warnings-irrelevantes","title":"2. Ignorando warnings irrelevantes","text":"<p><pre><code>warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n</code></pre> Esconde mensagens de alerta (warnings) do openpyxl para n\u00e3o poluir o terminal.</p>"},{"location":"automacoes/filtragem_excel/#3-carregando-o-excel","title":"3. Carregando o Excel","text":"<p><pre><code>file_path = \"./data/dataset.xlsx\"\ndf = pd.read_excel(file_path, sheet_name=\"Relat\u00f3rio 1\", skiprows=3)\ndf = df.iloc[:, 1:]\n</code></pre> - <code>file_path</code> \u2192 define o caminho do diret\u00f3rio do arquivo. - <code>skiprows=3</code> \u2192 pula as 3 primeiras linhas (cabecalhos extras). - <code>df.iloc[:, 1:]</code> \u2192 remove a primeira coluna (vazia ou in\u00fatil).</p>"},{"location":"automacoes/filtragem_excel/#4-criando-a-coluna-tipo","title":"4. Criando a coluna \"Tipo\"","text":"<p><pre><code>df = add_tipo_column(df)\n</code></pre> A fun\u00e7\u00e3o <code>add_tipo_column()</code>: - olha para <code>\"Elemento Item Despesa - C\u00f3digo\"</code> - associa o nome correspondente ao c\u00f3digo, criando nova coluna <code>\"Tipo\"</code> (ex: 3001 \u2192 \"Fornecimento\") - retorna o dataframe com a nova coluna</p>"},{"location":"automacoes/filtragem_excel/#5-filtrando-pelo-mes-anterior","title":"5. Filtrando pelo m\u00eas anterior","text":"<p><pre><code>past_month = datetime.now().month - 1\ndf_filtered = df[df[\"M\u00eas - Num\u00e9rico\"] == past_month]\n</code></pre> - descobre o m\u00eas anterior, - pega apenas as linhas cujo <code>\"M\u00eas - Num\u00e9rico\"</code> coincide.</p>"},{"location":"automacoes/filtragem_excel/#6-agrupando-resultados","title":"6. Agrupando resultados","text":"<p><pre><code>groups = df_filtered.groupby([\"Tipo\", \"Fonte Recurso - C\u00f3digo\"])\n</code></pre> - Aqui voc\u00ea divide os dados como se fossem \u201ccategorias\u201d, por exemplo:</p> Tipo Fonte Recurso - C\u00f3digo Fornecimento 100 Fornecimento 200 Servi\u00e7o 100 <ul> <li>Cada combina\u00e7\u00e3o vira uma aba diferente no Excel final.</li> </ul>"},{"location":"automacoes/filtragem_excel/#7-gerando-o-arquivo-final","title":"7. Gerando o arquivo final","text":"<p><pre><code>output_path = f\"./data/output_{past_month}.xlsx\"\nwith pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n    for k, ((tipo, fonte), group_df) in enumerate(groups):\n        sheet_name = f\"{tipo} - Fonte {fonte}\"\n        group_df.to_excel(writer, sheet_name=sheet_name, index=False)\n</code></pre> Para cada grupo: - cria uma aba com nome <code>\"Tipo - Fonte XYZ\"</code>. - exporta somente as linhas daquele grupo. O resultado final \u00e9 uma pasta de trabalho Excel contendo v\u00e1rias abas separadas por Tipo + Fonte.</p>"},{"location":"dashboards/dccc/intro/","title":"Dashboards PAC, Contratos, Conv\u00eanios e TDCOs","text":"<p>Esses pain\u00e9is foram desenvolvidos para consolidar e monitorar informa\u00e7\u00f5es financeiras referentes ao Planejamento Anual de Compras, Contrata\u00e7\u00f5es, Conv\u00eanios e Termos de Descentraliza\u00e7\u00e3o de Cr\u00e9dito Or\u00e7ament\u00e1rio em que a SEPLAG est\u00e1 envolvida. </p>"},{"location":"dashboards/dccc/intro/#objetivo","title":"Objetivo","text":"<p>O objetivo dos dashboards \u00e9 oferecer vis\u00e3o consolidada e atualizada das principais m\u00e9tricas financeiras relevantes aos gestores de programas e \u00e1reas contratantes, permitindo o acompanhamento da execu\u00e7\u00e3o or\u00e7ament\u00e1ria e a identifica\u00e7\u00e3o de gargalos em tempo real.</p>"},{"location":"dashboards/dccc/intro/#publico-alvo","title":"P\u00fablico-Alvo","text":"<ul> <li>Diretoria de Compras, Contratos e Conv\u00eanios (DCCC).</li> <li>Diretoria de Planejamento e Or\u00e7amento (DPO).</li> <li>Gestores de Programas e Projetos  </li> <li>\u00c1reas contratantes</li> </ul>"},{"location":"dashboards/dccc/intro/#fontes-de-dados","title":"Fontes de Dados","text":"Fonte Tipo Planilhas DCCC Excel (Sharepoint) Bases de dados extra\u00eddas via B.O (BI Corporativo - SAP) Excel (Sharepoint)"},{"location":"dashboards/dccc/intro/#acesso","title":"Acesso","text":"<p>O dashboard est\u00e1 dispon\u00edvel em:</p> <p>Dashboard PAC (Power BI) Dashboard Contratos, Conv\u00eanios e TDCOs(Power BI) </p>"},{"location":"dashboards/dccc/intro/#atualizacoes-e-manutencao","title":"Atualiza\u00e7\u00f5es e Manuten\u00e7\u00e3o","text":"<ul> <li>Os pain\u00e9is s\u00e3o atualizados diariamente, de hora em hora, das 10h-17h, via configura\u00e7\u00e3o do Modelo Sem\u00e2ntico do Power BI online. </li> <li>Al\u00e9m disso, para as bases extra\u00eddas do B.O, \u00e9 necess\u00e1ria uma configura\u00e7\u00e3o adicional para a atualiza\u00e7\u00e3o dos dados por meio de fluxo elaborado a partir de integra\u00e7\u00e3o entre o SAP e o Power Automate Web.</li> </ul>"},{"location":"dashboards/dccc/intro/#tecnologias-e-integracoes","title":"Tecnologias e Integra\u00e7\u00f5es","text":"<ul> <li>Power BI (Desktop e Online) \u2014 plotagem de visualiza\u00e7\u00f5es e atualiza\u00e7\u00e3o autom\u00e1tica dos dados</li> <li>Power Query / DAX \u2014 transforma\u00e7\u00e3o e modelagem de dados  </li> <li>SAP (B.O ou BI Corporativo) - gera\u00e7\u00e3o e envio de bases de dados</li> <li>Power Automate Web \u2014 integra\u00e7\u00e3o de bases de dados</li> </ul>"},{"location":"dashboards/dccc/intro/#contato-responsaveis","title":"Contato / Respons\u00e1veis","text":"<p>O desenvolvimento dos pain\u00e9is foi iniciado na Assessoria da SPF, mas a manuten\u00e7\u00e3o atual est\u00e1 a cargo da Diretoria de Compras, Contratos e Conv\u00eanios.  </p> <p>Para mais informa\u00e7\u00f5es ou suporte t\u00e9cnico, entre em contato com:</p> <ul> <li>Henrique Freitas Dias - Diretor de Compras, Contratos e Conv\u00eanios - henrique.freitas@planejamento.mg.gov.br  </li> <li>Diego Cesar Evangelista Ara\u00fajo - T\u00e9cnico da Diretoria de Compras, Contratos e Conv\u00eanios - diego.araujo@planejamento.mg.gov.br</li> </ul>"},{"location":"dashboards/dpo/fluxo_geral/","title":"Integra\u00e7\u00e3o das ferramentas e processamento dos dados","text":"<p>Para garantir o processamento e a atualiza\u00e7\u00e3o dos dados em tempo real, foi necess\u00e1rio definir um fluxo de ETL (Extract, Transform, Load) que englobasse todo o processo: download autom\u00e1tico da planilha, tratamento e modelagem dos dados via <code>pandas</code>, e carregamento dos resultados no painel do Power BI \u2014 tudo de forma totalmente automatizada.  </p> <p>Para isso, foram investigadas quais ferramentas melhor atenderiam a cada etapa e como integr\u00e1-las em um \u00fanico fluxo cont\u00ednuo. Em linhas gerais, s\u00e3o utilizadas as seguintes ferramentas, de forma integrada: Sharepoint, python, Github e PBI.</p> <p>Abaixo est\u00e1 uma explica\u00e7\u00e3o resumida de cada tecnologia envolvida e do fluxo geral da rotina. </p>"},{"location":"dashboards/dpo/fluxo_geral/#git-e-github","title":"Git e GitHub","text":"<p>Para saber o que \u00e9 o Git e o GitHub, leia aqui.</p> <p>O GitHub permite armazenar e versionar c\u00f3digo na nuvem, al\u00e9m de integrar ferramentas para automa\u00e7\u00e3o, como o GitHub Actions. O projeto desenvolvido utiliza o GitHub como reposit\u00f3rio central e controle de versionamento via Git.</p>"},{"location":"dashboards/dpo/fluxo_geral/#github-actions","title":"GitHub Actions","text":"<p>Para execu\u00e7\u00e3o autom\u00e1tica dos scripts foi utilizado o GitHub Actions.</p> <p>Para saber o que \u00e9 o GitHub Actions, leia aqui.</p> <p>As VMs disponibilizadas pelo GitHub Actions permitem a automa\u00e7\u00e3o completa dos scripts de download e tratamento de dados utilizados pela DPO. Ap\u00f3s o processamento, os arquivos <code>.csv</code> gerados s\u00e3o automaticamente commitados de volta ao reposit\u00f3rio, prontos para o carregamento no Power BI.</p>"},{"location":"dashboards/dpo/fluxo_geral/#secrets-e-personal-access-token-pat","title":"Secrets e Personal Access Token (PAT)","text":"<p>Por motivos de seguran\u00e7a, o reposit\u00f3rio GitHub foi configurado como privado, permitindo acesso apenas a usu\u00e1rios autorizados.</p> <p>Para que a rotina automatizada funcione corretamente \u2014 especialmente nos momentos em que a VM precisa enviar commits ao reposit\u00f3rio e carregar dados no Power BI \u2014 \u00e9 necess\u00e1rio o uso de credenciais de acesso.  </p> <p>Essas credenciais s\u00e3o configuradas por meio dos Secrets e Personal Access Token (PAT), definidos dentro do pr\u00f3prio GitHub. O passo a passo para cria\u00e7\u00e3o dessas credenciais est\u00e1 descrito neste guia.</p>"},{"location":"dashboards/dpo/fluxo_geral/#msal-e-office365-rest-python-client","title":"msal e Office365-REST-Python-Client","text":""},{"location":"dashboards/dpo/fluxo_geral/#download-automatico-da-planilha-do-sharepoint","title":"Download autom\u00e1tico da planilha do SharePoint","text":"<p>Uma das etapas mais importantes \u00e9 o download automatizado da planilha hospedada no SharePoint, utilizando Python.  </p> <p>Para isso, foram usadas as bibliotecas: <code>msal</code> e <code>Office365-REST-Python-Client</code>.</p> <p>Ambas s\u00e3o mantidas pela Microsoft e permitem a integra\u00e7\u00e3o com os servi\u00e7os do Office 365 via API, utilizando credenciais corporativas.</p> <p>Com elas, foi poss\u00edvel construir um script Python capaz de baixar automaticamente a planilha diretamente do SharePoint.</p>"},{"location":"dashboards/dpo/fluxo_geral/#criacao-do-access-token-e-refresh-token","title":"Cria\u00e7\u00e3o do Access Token e Refresh Token","text":"<p>Durante a elabora\u00e7\u00e3o do script de download, identificou-se um desafio: a necessidade de login manual com a conta organizacional. Isso impediria a execu\u00e7\u00e3o automatizada do fluxo completo.</p> <p>Para resolver esse problema, foi implementada uma autentica\u00e7\u00e3o autom\u00e1tica com o SharePoint, utilizando a gera\u00e7\u00e3o de Access Tokens e Refresh Tokens via <code>msal</code>.</p> <p>Para entender o que s\u00e3o Access Token e Refresh Token, leia aqui.</p> <p>Na primeira execu\u00e7\u00e3o do script, o login \u00e9 feito manualmente, e os tokens gerados s\u00e3o armazenados em um arquivo de cache (<code>msal_cache.bin</code>).  Esse arquivo \u00e9 ent\u00e3o commitado no reposit\u00f3rio privado do GitHub.</p> <p>Nas execu\u00e7\u00f5es seguintes, o fluxo usa o Refresh Token armazenado para gerar automaticamente um novo Access Token, sem necessidade de login manual. Assim, o GitHub Actions executa todo o processo de autentica\u00e7\u00e3o e download sem interven\u00e7\u00e3o humana.</p> <p>O Refresh Token tamb\u00e9m possui tempo de expira\u00e7\u00e3o. Ainda n\u00e3o h\u00e1 uma defini\u00e7\u00e3o exata sobre sua validade, portanto \u00e9 necess\u00e1rio monitorar as execu\u00e7\u00f5es dos workflows do GitHub Actions e regerar manualmente os tokens quando necess\u00e1rio.</p>"},{"location":"dashboards/dpo/fluxo_geral/#obtencao-dos-scripts-de-download-credenciais-organizacionais-api-sharepoint-e-geracoes-dos-tokens","title":"Obten\u00e7\u00e3o dos scripts de download, credenciais organizacionais API sharepoint e gera\u00e7\u00f5es dos Tokens","text":"<p>Deve-se ressaltar que a elabora\u00e7\u00e3o da rotina descrita aqui contou com suporte da equipe da Assessoria da Intelig\u00eancia de Dados da Subsecretaria Central de Planejamento e Or\u00e7amento da SEPLAG/MG. Al\u00e9m do aux\u00edlio para projetar e implementar a rotina, foi cedido o script sharepoint_utils.py que cont\u00e9m as fun\u00e7\u00f5es de gerar os Tokens e de realizar download e upload de arquivos do Sharepoint, al\u00e9m das credenciais organizacionais de acesso aos servi\u00e7os do Office 365 via API. </p> <p>Detalhar que a fun\u00e7\u00e3o de gerar os Tokens teve que ser modificada para funcionar</p>"},{"location":"dashboards/dpo/fluxo_geral/#fluxo-geral","title":"Fluxo geral","text":"flowchart TD A[\"In\u00edcio do processo\"] --&gt; B[\"Execu\u00e7\u00e3o manual inicial do script Pythonde download da planilha da DPO\"] B --&gt; C[\"Login manual na conta organizacional (MSAL)\"] C --&gt; D[\"Gera\u00e7\u00e3o do Access Token e Refresh Token\"] D --&gt; E[\"Armazenamento em cache local (msal_cache.bin)\"] E --&gt; F[\"Commit e push manual do cache para o reposit\u00f3rio privado do GitHub\"] F --&gt; G[\"Execu\u00e7\u00e3o autom\u00e1tica da rotina via GitHub Actions\"] G --&gt; H[\"Download automatizado da planilha do SharePoint\"] H --&gt; I[\"Tratamento e modelagem dos dados com pandas\"] I --&gt; J[\"Exporta\u00e7\u00e3o dos CSVs tratados\"] J --&gt; K[\"Commit autom\u00e1tico dos CSVs no reposit\u00f3rio via workflow.yml\"] K --&gt; L[\"Carregamento dos dados no Power BI\"] L --&gt; M[\"Fim do processo\"]"},{"location":"dashboards/dpo/fluxo_geral/#comentarios-sobre-o-fluxo","title":"Coment\u00e1rios sobre o fluxo","text":"<ul> <li>A primeira execu\u00e7\u00e3o requer login, commit e push manualmente para gerar os tokens iniciais e o arquivo cache.</li> <li>As execu\u00e7\u00f5es seguintes s\u00e3o 100% autom\u00e1ticas, at\u00e9 o vencimento do Refresh Token.  </li> <li>O GitHub Actions atua como o motor de automa\u00e7\u00e3o, orquestrando todo o fluxo ETL.</li> </ul>"},{"location":"dashboards/dpo/intro/","title":"Dashboard Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria","text":"<p>Esse painel foi desenvolvido para consolidar informa\u00e7\u00f5es a respeito da Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria da SEPLAG.</p>"},{"location":"dashboards/dpo/intro/#objetivo","title":"Objetivo","text":"<p>Oferecer informa\u00e7\u00f5es relevantes e atualizadas, a fim de subsidiar a melhoria da tomada de decis\u00e3o e permitir o melhor acompanhamento da execu\u00e7\u00e3o or\u00e7ament\u00e1ria pelas \u00e1reas t\u00e9cnicas.</p>"},{"location":"dashboards/dpo/intro/#publico-alvo","title":"P\u00fablico-alvo","text":"<ul> <li>Diretoria de Planejamento e Or\u00e7amento (DPO).</li> <li>Subsecret\u00e1rios(as) e Secret\u00e1rio(a).</li> <li>\u00c1reas respons\u00e1veis pelas a\u00e7\u00f5es or\u00e7ament\u00e1rias.</li> </ul>"},{"location":"dashboards/dpo/intro/#fontes-de-dados","title":"Fontes de Dados","text":"Fonte Tipo Planilha DPO Excel (Sharepoint) Bases de dados tratadas via Python + Github Actions + msal .csv"},{"location":"dashboards/dpo/intro/#acesso","title":"Acesso","text":"<p>O dashboard est\u00e1 dispon\u00edvel em:  Dashboard Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria (Power BI) Reposit\u00f3rio github</p>"},{"location":"dashboards/dpo/intro/#atualizacoes-e-manutencao","title":"Atualiza\u00e7\u00f5es e Manuten\u00e7\u00e3o","text":"<ul> <li>Os pain\u00e9is s\u00e3o atualizados diariamente, de hora em hora, das 10h-17h.</li> <li>Configura\u00e7\u00e3o do Modelo Sem\u00e2ntico do Power BI online e dos cron jobs do github actions.</li> </ul>"},{"location":"dashboards/dpo/intro/#tecnologias-e-integracoes","title":"Tecnologias e integra\u00e7\u00f5es","text":"<ul> <li>Power BI (Desktop e Online) \u2014 plotagem de visualiza\u00e7\u00f5es e atualiza\u00e7\u00e3o autom\u00e1tica dos dados</li> <li>Python (pandas) / Power Query / DAX \u2014 transforma\u00e7\u00e3o e modelagem de dados  </li> <li>github actions \u2014 execu\u00e7\u00e3o autom\u00e1tica de scripts para tratamento e e atualiza\u00e7\u00e3o das bases.</li> </ul>"},{"location":"dashboards/dpo/passo_a_passo/","title":"Download manual da planilha do Sharepoint e gera\u00e7\u00e3o dos Tokens","text":"<p>Como citado anteriormente, a primeira execu\u00e7\u00e3o do script dever\u00e1 ser feita de forma manual, para gera\u00e7\u00e3o dos Tokens de acesso atrav\u00e9s do arquivo de cache.</p> <p>As execu\u00e7\u00f5es seguintes s\u00e3o feitas de forma automatizada pelo Github Actions, a partir do Access e Refresh Tokens gerados e armazenados e commitados para o Github.</p> <p>download.py <pre><code>from sharepoint_utils import download_sharepoint_file, get_sharepoint_token\n\nbase_url = \"https://cecad365.sharepoint.com/sites/DPO\"\nfolder_path = \"/sites/DPO/Documentos Compartilhados/1 Or\u00e7amento/1 Acompanhamento da Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria/Execu\u00e7\u00e3o 2025\"\n\ndownload_sharepoint_file(base_url, folder_path, \"Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx\", \"Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx\")\n</code></pre></p> <p>sharepoint_utils.py <pre><code>def download_sharepoint_file(base_url, folder_path, file_name, local_filename):\n    \"\"\"\n    Downloads a file from SharePoint using dynamic components for the URL.\n\n    Args:\n        base_url (str): The base SharePoint site URL (e.g., 'https://cecad365.sharepoint.com/sites/Splor').\n        folder_path (str): The relative path to the folder containing the file (e.g., '/Documentos Compartilhados/General').\n        file_name (str): The name of the file to download (e.g., 'datamart.xlsx').\n        local_filename (str): The local path where the file will be saved.\n\n    Returns:\n        bool: True if the file was downloaded successfully, False otherwise.\n    \"\"\"\n</code></pre></p> <ul> <li> <p>Como par\u00e2metros da fun\u00e7\u00e3o s\u00e3o chamados a url do site da organiza\u00e7\u00e3o no Sharepoint e o caminho da pasta em que o arquivo est\u00e1 salvo. Em seguida, de acordo com as anota\u00e7\u00f5es da fun\u00e7\u00e3o, define-se o nome do arquivo que ser\u00e1 baixado e o caminho local em que o arquivo ser\u00e1 baixado.</p> </li> <li> <p>Na chamada da fun\u00e7\u00e3o de download, primeiro \u00e9 chamada a fun\u00e7\u00e3o de gera\u00e7\u00e3o dos Tokens, para validar o acesso e realizar o credenciamento. \u00c9 a fun\u00e7\u00e3o <code>get_sharepoint_token()</code>.</p> </li> </ul> <p>sharepoint_utils.py <pre><code>config = {\n  \"authority\": \"https://login.microsoftonline.com/e5d3ae7c-9b38-48de-a087-f6734a287574\",\n  \"client_id\": \"d44a05d5-c6a5-4bbb-82d2-443123722380\",\n  \"scope\": [\"https://cecad365.sharepoint.com/.default\"], #[\"Group.ReadWrite.All\"],\n  \"username\": \"username@ca.mg.gov.br\",\n  \"endpoint\": \"https://login.microsoftonline.com/common/oauth2/v2.0/authorize\"\n}\n</code></pre> - Antes de detalhar a fun\u00e7\u00e3o, \u00e9 preciso explicar a vari\u00e1vel config, do tipo dicion\u00e1rio. A \u00fanica informa\u00e7\u00e3o que dever\u00e1 ser alterada aqui \u00e9 a vinculada \u00e0 chave de username, com os dados da conta organizacional do usu\u00e1rio respons\u00e1vel pela execu\u00e7\u00e3o da rotina.  - Essa vari\u00e1vel ser\u00e1 utilizada pela fun\u00e7\u00e3o get_sharepoint_token() para o credenciamento e utiliza\u00e7\u00e3o da API do Office 365.</p> <p>sharepoint_utils.py <pre><code>def get_sharepoint_token():\n    # \u2705 Load or create token cache\n    cache = msal.SerializableTokenCache()\n    if os.path.exists(CACHE_FILE):\n        cache.deserialize(open(CACHE_FILE, \"r\").read())\n</code></pre> - Inicialmente tenta-se ler o arquivo cache em que estar\u00e3o armazenadas as informa\u00e7\u00f5es de acesso do usu\u00e1rio e os Tokens de credenciamento, o Access Token e o Refresh Token. Caso esse arquivo exista, ele \u00e9 lido e armazenado na vari\u00e1vel cache.</p> <p><pre><code>    app = msal.PublicClientApplication(\n        config[\"client_id\"], authority=config[\"authority\"], token_cache=cache\n        )\n</code></pre> - O app \u00e9 ent\u00e3o carregado com, al\u00e9m das credenciais da organiza\u00e7\u00e3o, as informa\u00e7\u00f5es presentes no cache, se existirem.</p> <p><pre><code>    # We now check the cache to see\n    # whether we already have some accounts that the end user already used to sign in before.\n    accounts = app.get_accounts(username=config.get(\"username\"))\n    print(accounts)\n</code></pre> - A pr\u00f3xima etapa \u00e9 checar se a credencial do usu\u00e1rio corresponde a algum dos usu\u00e1rios listados no cache. Ou seja, se o usu\u00e1rio, conforme inserido na vari\u00e1vel username \u00e9 autenticado e possui algum Token correspondente. Caso seja, a vari\u00e1vel account ser\u00e1 carregada com os dados desse usu\u00e1rio e caso n\u00e3o, ela ser\u00e1 vazia. \u00c9 v\u00e1lido ressaltar que na primeira execu\u00e7\u00e3o do script, essa vari\u00e1vel accounts ser\u00e1 vazia.</p> <p><pre><code>    if accounts:\n        result = app.acquire_token_silent(config[\"scope\"], account=accounts[0])\n</code></pre> - Caso seja encontrado os dados daquele usu\u00e1rio no cache, o usu\u00e1rio \u00e9 autenticado, ou seja, o Access Token \u00e9 adquirido silenciosamente a partir do Refresh Token. </p> <p>**PAREI AQUI Se n\u00e3o conseguir gerar o Token silenciosamente, o navegador \u00e9 aberto e pede-se ao usu\u00e1rio para logar na sua conta e, assim, se credenciar interativamente, gerando um novo Access Token e Refresh Token.</p> <pre><code>if not result:\n    # So no suitable token exists in cache. Let's get a new one from Azure AD.\n    result = app.acquire_token_interactive(scopes=config[\"scope\"])\n\n# \u2705 Save updated cache\nif cache.has_state_changed:\n    with open(CACHE_FILE, \"w\") as f:\n        f.write(cache.serialize())\n    print(\"\ud83d\udcbe Token cache saved.\")\n\nif \"access_token\" in result:\n    #print(result)  # Yay!\n    return result\nelse:\n    print(result.get(\"error\"))\n    print(result.get(\"error_description\"))\n    print(result.get(\"correlation_id\"))  # You may need this when reporting a bug\n</code></pre> <p>```   - Abaixo segue o detalhamento do script de download da planilha e do script de gera\u00e7\u00e3o dos Tokens necess\u00e1rios para o credenciamento no Sharepoint.     </p>"},{"location":"dashboards/dpo/passo_a_passo/#fluxo-geral","title":"Fluxo geral","text":"<p>Para que o processamento e a atualiza\u00e7\u00e3o dos dados ocorram em tempo real, foi necess\u00e1rio primeiro garantir que o download da planilha fosse executado de forma autom\u00e1tica. Assim, quando o script de tratamento fosse iniciado, ele utilizaria sempre os dados mais recentes dispon\u00edveis.  Para cria\u00e7\u00e3o dessa rotina foi utilizada bibliotecas da Microsoft, em conjunto com scripts e credenciais fornecidos pelos t\u00e9cnicos da Subsecretaria Central de Planejamento e Or\u00e7amento (SPLOR/SEPLAG), permitindo o download da planilha e execu\u00e7\u00e3o dos scripts de tratamento de forma automatizadaq.  No entanto, a execu\u00e7\u00e3o dessa rotina de forma totalmente automatizada em Python apresentava uma grande limita\u00e7\u00e3o inicialmente: o login no SharePoint exigia autentica\u00e7\u00e3o manual a cada execu\u00e7\u00e3o, o que inviabilizava a automa\u00e7\u00e3o completa do download.  Para contornar essa restri\u00e7\u00e3o, foi criado fluxo de autentica\u00e7\u00e3o seguro, a partir da  implementa\u00e7\u00e3o de uma l\u00f3gica para gera\u00e7\u00e3o autom\u00e1tica de Access Token a partir de um Refresh Token, obtido ap\u00f3s um login manual inicial. O cache de acesso \u00e9 ent\u00e3o versionado via commit e push no reposit\u00f3rio GitHub, garantindo que a autentica\u00e7\u00e3o seja reutilizada nas execu\u00e7\u00f5es seguintes. </p> <p>Para entender o que s\u00e3o Access Token e Refresh Token, clique aqui.</p> <p>Por quest\u00f5es de seguran\u00e7a, o reposit\u00f3rio GitHub foi configurado como privado.</p> <p>Nos pr\u00f3ximos t\u00f3picos, ser\u00e3o explicados em detalhes o fluxo de autentica\u00e7\u00e3o e os scripts respons\u00e1veis pelo download automatizado.</p>"},{"location":"dashboards/dpo/passo_a_passo/#primeira-execucao-do-script-de-download-execucao-manual","title":"Primeira execu\u00e7\u00e3o do script de download (execu\u00e7\u00e3o manual)","text":"<ul> <li>A primeira execu\u00e7\u00e3o dever\u00e1 ser realizada de forma manual para gera\u00e7\u00e3o do Access Token e do Refresh Token salvos no arquivo cache. </li> <li>O arquivo cache ent\u00e3o dever\u00e1 commitado para o repo do Github.  <code>mermaid flowchart TD     A([In\u00edcio]) --&gt; B[Download manual da planilha&lt;br/&gt;&lt;small&gt;&lt;i&gt;Execu\u00e7\u00e3o manual do script Python de download&lt;/i&gt;&lt;/small&gt;]     B --&gt; C[Autentica\u00e7\u00e3o no SharePoint&lt;br/&gt;&lt;small&gt;&lt;i&gt;Login inicial manual&lt;/i&gt;&lt;/small&gt;]     C --&gt; D[Gera\u00e7\u00e3o de Refresh Token e Access Token]     D --&gt; E[Salvar tokens em cache local]     E --&gt; F[Commit e push do cache de acesso&lt;br/&gt;para reposit\u00f3rio privado do GitHub]     F --&gt; G([Fim])</code></li> </ul>"},{"location":"dashboards/dpo/passo_a_passo/#execucoes-automatizadas","title":"Execu\u00e7\u00f5es automatizadas","text":"<ul> <li>A partir do arquivo cache commitado para o github torna-se poss\u00edvel a execu\u00e7\u00e3o autom\u00e1tica do script de download por conta da l\u00f3gica de gera\u00e7\u00e3o de Access Token a partir de Refresh Token.  </li> </ul> <p>autentica\u00e7\u00e3o autom\u00e1tica das VMs do por conta da l\u00f3gica de gera\u00e7\u00e3o de Access Token a partir de Refresh Token.</p>"},{"location":"dashboards/dpo/pre_processamento_dados/","title":"Pr\u00e9-processamento dos dados","text":"<p>Como detalhado nas se\u00e7\u00f5es anteriores, devido \u00e0 complexidade da planilha utilizada no dia-a-dia de trabalho da DPO e por limita\u00e7\u00f5es do PBI ao processar o grande volume de dados foi necess\u00e1rio realizar o tratamento inicial dos dados utilizando outra solu\u00e7\u00e3o que n\u00e3o fosse o Power Query ou o DAX. </p> <p>Para isso, foi feito um pr\u00e9-processamento por meio do python, utilizando a lib do <code>pandas</code>, amplamente utilizada para leitura, limpeza, transforma\u00e7\u00e3o e an\u00e1lise de dados.  </p>"},{"location":"dashboards/dpo/pre_processamento_dados/#acesso","title":"Acesso","text":"<ul> <li>Para acessar os scripts, acesse o repo pelo github. <p>Caso n\u00e3o consiga visualizar o repo, pe\u00e7a para o respons\u00e1vel adicion\u00e1-lo como colaborador.</p> </li> </ul>"},{"location":"dashboards/dpo/pre_processamento_dados/#pre-processamento-da-planilha-da-dpo-process_datacsv","title":"Pr\u00e9-processamento da planilha da DPO (process_data.csv)","text":""},{"location":"dashboards/dpo/pre_processamento_dados/#etapa-1-filtrar-abas-relevantes","title":"Etapa 1 - Filtrar abas relevantes","text":"<ul> <li>A fun\u00e7\u00e3o <code>get_tables()</code> identifica quais abas da planilha cont\u00eam dados v\u00e1lidos.</li> <li>Ela ignora abas desnecess\u00e1rias como \u201cMenu\u201d e mant\u00e9m apenas aquelas com 4 caracteres.</li> </ul> <p><pre><code>def get_tables(excel_file):\n  # Get all sheet names\n  xls = pd.ExcelFile(excel_file)\n  sheet_names = xls.sheet_names\n  filtered_sheets = []\n  # Filter sheet names:\n  # - Keep only 4-character names\n  # - Remove 'Menu'\n  # - Remove names starting with '44'\n  for sheet in sheet_names:\n      if len(sheet) == 4 and sheet not in ['Menu', '44xx', '4488', '4489']:\n          filtered_sheets.append(sheet)\n\n  return filtered_sheets\n</code></pre> - Sa\u00edda: uma lista de nomes de abas filtradas.</p>"},{"location":"dashboards/dpo/pre_processamento_dados/#etapa-2-processar-abas-textuais","title":"Etapa 2 - Processar abas textuais","text":"<ul> <li>A fun\u00e7\u00e3o <code>process_tables_text()</code> trata colunas de texto, como \u201cObjeto\u201d e \u201cPrograma\u00e7\u00e3o\u201d. Principais a\u00e7\u00f5es</li> <li>Encontra a linha onde come\u00e7a a tabela (procura \u201cPrograma\u00e7\u00e3o\u201d).</li> <li>Remove linhas e colunas extras.</li> <li>\"Limpa\" a coluna \"Objeto\" para evitar erros de atualiza\u00e7\u00e3o por conta do preenchimento dos dados da planilha pelos t\u00e9cnicos (padroniza os textos -&gt; tudo em min\u00fasculo, tira acentos e pontua\u00e7\u00e3o, etc).</li> <li>Remove linhas baseado em valores espec\u00edficos de GMIFP.</li> </ul> <p><pre><code>def process_tables_text(excel_file, filtered_sheets):\n  treated_dfs = []\n  removed_ids = []\n  df = pd.DataFrame()\n\n  for sheet in filtered_sheets:\n      df = pd.read_excel(excel_file, sheet_name=sheet)\n      # Find the row index where first column contains 'Programa\u00e7\u00e3o'\n      target_row = df[df.iloc[:, 0].str.contains('Programa\u00e7\u00e3o', case=False, na=False)].index[0]\n      # Get all data starting from the target row\n      df = df.iloc[target_row:]\n      # Drop rows with empty values in the first column\n      df = df.dropna(subset=[df.columns[0]])\n      # Drop last row\n      df = df[:-1]\n      # Keep only the first 9 columns\n      df = df.iloc[:, :9]\n      # Promote first row to headers\n      df.columns = df.iloc[0]\n      df = df.iloc[1:]  # Remove the first row since it's now the header\n      # Add sheet name as 'A\u00e7\u00e3o'\n      df['A\u00e7\u00e3o'] = sheet\n\n      # Cleans column Objeto\n      df[\"Objeto\"] = (\n          df[\"Objeto\"]\n          .fillna(\"\")\n          .str.strip()\n          .str.lower()\n          .apply(lambda x: re.sub(r'[^\\w\\s]', \" \", x, flags=re.UNICODE))\n          .str.split()\n          .str.join(\" \")\n          .str.title()\n      )\n\n      #GMIFPs to remove\n      gmis_to_remove = [\"1.90.0.10.1\", \"1.91.0.10.1\", \"3.90.0.10.7\"]\n\n      # Get which rows will be removed\n      rows_to_remove = df[df['GMIFP'].isin(gmis_to_remove)]\n\n      # Extract the IDs of those rows for later processing by process_tables_num\n      removed_ids.extend(rows_to_remove['Programa\u00e7\u00e3o'].tolist())\n\n      # Effectively remove rows from df by GMIFP\n      df = df[~df['GMIFP'].isin(gmis_to_remove)]\n\n      print(\"Tratando aba:\", sheet)\n      treated_dfs.append(df)\n\n  if treated_dfs:\n    final_df = pd.concat(treated_dfs, ignore_index=True)\n    output_file = 'processed_data_text.csv'\n    final_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n    print(f\"\\nData exported to {output_file}\")\n\n    return removed_ids\n</code></pre> - Sa\u00edda: exporta os dados tratados para <code>processed_data_text.csv</code> e retorna uma lista de IDs das linhas removidas.</p>"},{"location":"dashboards/dpo/pre_processamento_dados/#etapa-3-processar-abas-numericas","title":"Etapa 3 \u2014 Processar abas num\u00e9ricas","text":"<ul> <li>A fun\u00e7\u00e3o <code>process_tables_num()</code> trata os valores financeiros.</li> <li>Ela usa os IDs removidos na etapa anterior para excluir registros indesejados. Principais a\u00e7\u00f5es</li> <li>Localiza a linha \u201cPrograma\u00e7\u00e3o\u201d e ajusta o in\u00edcio da tabela</li> <li>Remove colunas desnecess\u00e1rias (como \u201cTOTAIS\u201d)</li> <li>Transp\u00f5e os dados (linhas \u2194 colunas) e em seguida faz o unpivot e pivot para reorganizar os valores. <p>O passo 3 \u00e9 imprescind\u00edvel para associar corretamente o n\u00famero de programa\u00e7\u00e3o aos meses e \u00e0s respectivas colunas de valores, quais sejam \"Descentralizado\", \"Empenhado\", \"Liquidado\", \"Programado\" e \"Realizado\".</p> </li> <li>Converte colunas para n\u00fameros.</li> <li>Remove as linhas com os respectivos IDs do passo anterior (associados aos GMIFPs que devem ser removidos).</li> </ul> <p><pre><code>def process_tables_num(excel_file, filtered_sheets, removed_ids):\n  treated_dfs = []\n\n  for worksheet in filtered_sheets:\n      df = pd.read_excel(excel_file, sheet_name=worksheet)\n\n      # 1. Find the row index where first column contains 'programa\u00e7\u00e3o'\n      target_row = df[df.iloc[:, 0].str.contains('Programa\u00e7\u00e3o', case=False, na=False)].index[0]\n\n      # 2. Filter the DataFrame to start 2 rows before the target row\n      df = df.iloc[target_row-2:]\n\n      # 3. Remove rows where first column is null, but keep the first row\n      first_row = df.iloc[0:1]  # Keep first row\n      rest_of_df = df.iloc[1:].dropna(subset=[df.columns[0]])  # Remove nulls from rest\n      df = pd.concat([first_row, rest_of_df])  # Combine them back\n\n      # 4. Remove the last row\n      df = df.iloc[:-1]\n\n      # 5. Find the column with 'TOTAIS' in the second row and remove it and all columns after it\n      total_col_idx = df.iloc[1].str.contains('TOTAIS', case=False, na=False).idxmax()\n      total_col_num = df.columns.get_loc(total_col_idx)\n      df = df.iloc[:, :total_col_num]  # Keep all columns up to (but not including) the TOTAIS column\n\n      # 6. Remove columns 2 through 9\n      df = pd.concat([df.iloc[:, 0:1], df.iloc[:, 9:]], axis=1)\n\n      # 7. Transpose the DataFrame\n      df = df.transpose()\n\n      # 8. Remove the second column\n      df = df.drop(df.columns[1], axis=1)\n\n      # 9. Promote first row to headers\n      df.columns = df.iloc[0]  # Set column names to first row\n      df = df.iloc[1:]  # Remove the first row since it's now the header\n\n      # 10. Rename the first two columns\n      df = df.rename(columns={df.columns[0]: 'M\u00eas', df.columns[1]: 'Status'})\n\n      # 11. Unpivot the DataFrame\n      id_vars = ['M\u00eas', 'Status']  # Columns to keep as identifiers\n      df = pd.melt(df, \n                  id_vars=id_vars,\n                  var_name='Programa\u00e7\u00e3o',  # Name for the column containing the former column headers\n                  value_name='Valor')    # Name for the column containing the values\n\n      # 12. Add A\u00e7\u00e3o column with worksheet name\n      df['A\u00e7\u00e3o'] = worksheet\n\n      # 13. Pivot the table back using Status and Valor\n      df = df.pivot(index=['M\u00eas', 'Programa\u00e7\u00e3o', 'A\u00e7\u00e3o'], \n                  columns='Status', \n                  values='Valor').reset_index()\n\n      # 14. Force columns to be number types (filter unexpected characters in number columns)\n      columns_to_clean = ['Descentralizado', 'Empenhado', 'Liquidado', \"Programado\", \"Realizado\"]\n\n      for col in columns_to_clean:\n          # Remove tudo que n\u00e3o \u00e9 n\u00famero e converte para float, ou NaN se falhar\n          df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n\n      # 15. Remove rows where gmifp == [\"1.90.0.10.1\", \"1.91.0.10.1\", \"3.90.0.10.7\"] by n. Programa\u00e7\u00e3o\n      df = df[~df['Programa\u00e7\u00e3o'].isin(removed_ids)]\n\n      # Add to list of DataFrames\n      treated_dfs.append(df)\n      print(f\"Processed worksheet: {worksheet}\")\n\n  # Combine all DataFrames\n  if treated_dfs:\n      final_df = pd.concat(treated_dfs, ignore_index=True)\n\n      # Export to CSV\n      output_file = 'processed_data_num.csv'\n      final_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n      print(f\"\\nData exported to {output_file}\")\n</code></pre> - Sa\u00edda: exporta o resultado para <code>processed_data_num.csv</code>.</p>"},{"location":"dashboards/dpo/pre_processamento_dados/#execucao-do-script","title":"Execu\u00e7\u00e3o do Script","text":"<ul> <li>Por fim, basta passar o nome da planilha e chamar as fun\u00e7\u00f5es na sequ\u00eancia:</li> </ul> <pre><code>excel_file = 'Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria 2025.xlsx'\n\nfiltered_sheets = get_tables(excel_file)\nremoved_ids = process_tables_text(excel_file, filtered_sheets)\nprocess_tables_num(excel_file, filtered_sheets, removed_ids)\n</code></pre>"},{"location":"dashboards/dpo/pre_processamento_dados/#resultado-esperado","title":"Resultado esperado","text":"<ul> <li><code>processed_data_text.csv</code></li> <li><code>processed_data_num.csv</code></li> </ul> <p>Esses arquivos ser\u00e3o integrados via GitHub Actions e ent\u00e3o usados pelo Power BI.</p>"},{"location":"dashboards/dpo/projeto/","title":"Projeto e Cronograma Painel DPO","text":"<p>Este projeto se refere \u00e0 elabora\u00e7\u00e3o do Painel (Dashboard) de Business Intelligence (BI) da Diretoria de Planejamento e Or\u00e7amento (DPO) da Superintend\u00eancia de Planejamento e Finan\u00e7as (SPF) da Subsecretaria de Gest\u00e3o e Finan\u00e7as (SUBGEF) da Secretaria de Planejamento e Gest\u00e3o do Estado de Minas Gerais (SEPLAG-MG).</p> <p>O principal fator identificado durante a investiga\u00e7\u00e3o realizada foi a elevada complexidade t\u00e9cnica associada \u00e0 manuten\u00e7\u00e3o e ao uso da planilha atualmente em vigor. Utilizada h\u00e1 v\u00e1rios anos, essa planilha passou por sucessivos ajustes e adapta\u00e7\u00f5es, o que comprometeu sua estrutura tanto como base de dados quanto como ferramenta de consulta.  </p> <p>Al\u00e9m disso, verificou-se que a forma de organiza\u00e7\u00e3o dos dados dificulta a compreens\u00e3o por parte das \u00e1reas t\u00e9cnicas externas, que enfrentam obst\u00e1culos para acessar as informa\u00e7\u00f5es de maneira clara e visualmente organizada. Isso evidencia uma lacuna entre a forma como os dados est\u00e3o dispostos e sua efetiva utilidade no apoio \u00e0s demandas dessas \u00e1reas.  </p> <p>Os dados presentes na planilha s\u00e3o consumidos e utilizados por agentes de diversos n\u00edveis organizacionais \u2014 t\u00e9cnicos da DPO, \u00e1reas demandantes e tomadores de decis\u00e3o. Portanto, proporcionar uma comunica\u00e7\u00e3o clara e acess\u00edvel \u00e9 fundamental.  </p> <p>Dessa forma, foi identificada a necessidade de melhoria na representa\u00e7\u00e3o e visualiza\u00e7\u00e3o dos dados utilizados no contexto de trabalho da Diretoria, por meio do desenvolvimento de um dashboard de Business Intelligence.</p>"},{"location":"dashboards/dpo/projeto/#objetivos","title":"Objetivos","text":"<p>Este projeto possui os seguintes objetivos:</p> <ul> <li>Subsidiar o desenvolvimento de um dashboard de BI para apoiar processos de trabalho e tomada de decis\u00e3o.  </li> <li>Documentar o processo de trabalho realizado pelos t\u00e9cnicos desta Assessoria, servindo como metodologia base para futuros pain\u00e9is ou automa\u00e7\u00f5es.  </li> <li>Apoiar a elabora\u00e7\u00e3o de um cronograma com prazos, metas e entregas previstas.</li> </ul> <p>O resultado esperado \u00e9 a entrega de um painel validado pelos usu\u00e1rios e em conformidade com as demandas operacionais da DPO, das \u00e1reas externas e dos tomadores de decis\u00e3o.</p>"},{"location":"dashboards/dpo/projeto/#metodologia","title":"Metodologia","text":"<p>Para a defini\u00e7\u00e3o do escopo do projeto, foram realizadas reuni\u00f5es com a DPO e estudo da planilha Excel utilizada como fonte de dados.  </p> <p>O painel ser\u00e1 estruturado para contemplar duas abas principais:</p> <ul> <li>Informa\u00e7\u00f5es T\u00e9cnicas: voltadas ao contexto operacional da DPO, para acompanhamento di\u00e1rio.  </li> <li>Informa\u00e7\u00f5es Gerenciais: voltadas \u00e0 avalia\u00e7\u00e3o estrat\u00e9gica e tomada de decis\u00e3o dos gestores.  </li> </ul> <p>A metodologia de desenvolvimento adotada foi incremental, com entregas evolutivas em fases e ciclos, atendendo \u00e0s seguintes particularidades:</p> <ul> <li>Entregas parciais e r\u00e1pidas  </li> <li>Engajamento cont\u00ednuo dos envolvidos  </li> <li>Redu\u00e7\u00e3o de retrabalho  </li> </ul> <p>Ferramentas utilizadas: - Power BI, Power Query, Excel Online, SharePoint - Python (<code>msal</code>, <code>Office365-REST-Python-Client</code>, <code>pandas</code>) - GitHub, GitHub Actions - Git Bash (recomendado) - VS Code (recomendado)</p>"},{"location":"dashboards/dpo/projeto/#fases-e-ciclos","title":"Fases e Ciclos","text":"<p>Fase 0 \u2014 Alinhamento e Planejamento Inicial</p> <ul> <li>Conversas iniciais com a DPO (mapeamento do processo, entendimento do problema)  </li> <li>Estudo inicial da fonte de dados  </li> <li>Defini\u00e7\u00e3o das fases do projeto e cronograma  </li> </ul> <p>Ao final, foi decidido dividir o desenvolvimento do painel em tr\u00eas fases: 1. Aba t\u00e9cnica (informa\u00e7\u00f5es para \u00e1reas e t\u00e9cnicos da DPO) 2. Aba gerencial (informa\u00e7\u00f5es para gestores) 3. Testes e homologa\u00e7\u00e3o final  </p> <p>Fase 1 e Fase 2 \u2014 Desenvolvimento das Abas</p> <p>Etapas por ciclo:</p> <ol> <li>Conversas com a DPO \u2014 defini\u00e7\u00e3o de escopo, layout e prioridades.  </li> <li>Modelagem de Dados e ETL \u2014 extra\u00e7\u00e3o, transforma\u00e7\u00e3o e padroniza\u00e7\u00e3o.  </li> <li>Desenvolvimento do Painel \u2014 constru\u00e7\u00e3o de indicadores e visualiza\u00e7\u00f5es.  </li> <li>Valida\u00e7\u00e3o \u2014 testes de consist\u00eancia e coleta de feedbacks.  </li> </ol> <p>Fase 3 \u2014 Valida\u00e7\u00e3o e Homologa\u00e7\u00e3o Final</p> <p>Essa fase garante estabilidade t\u00e9cnica, confiabilidade dos dados e boa usabilidade. Recomenda-se realizar um teste piloto com usu\u00e1rios-chave de diferentes perfis (analistas, gestores, operacionais).</p> <p>Etapas: - Coleta de feedback sobre clareza, utilidade e erros. - Ajustes e homologa\u00e7\u00e3o final antes da libera\u00e7\u00e3o.  </p>"},{"location":"dashboards/dpo/projeto/#cronograma","title":"Cronograma","text":"Fase Atividade Respons\u00e1veis Prazo Estimado Fase 0 Reuni\u00f5es iniciais com a DPO Assessoria SPF + DPO 2 dias Fase 0 Estudo da fonte de dados Assessoria SPF 4 dias Fase 0 Defini\u00e7\u00e3o de fases e cronograma Assessoria SPF 3 dias Fase 0 Valida\u00e7\u00e3o do planejamento Assessoria SPF + DPO 1 dia Fase 1 Conversas com DPO (escopo, layout) Assessoria SPF + DPO 1 dia (por ciclo) Fase 1 Modelagem de dados e ETL Assessoria SPF 3 dias (por ciclo) Fase 1 Desenvolvimento do painel Assessoria SPF 2 dias (por ciclo) Fase 1 Valida\u00e7\u00e3o com t\u00e9cnicos DPO + Assessoria SPF 1 dia (por ciclo) Fase 2 Conversas com gestores Assessoria SPF + DPO 1 dia (por ciclo) Fase 2 Modelagem de dados gerencial Assessoria SPF 3 dias (por ciclo) Fase 2 Desenvolvimento dos pain\u00e9is gerenciais Assessoria SPF 2 dias (por ciclo) Fase 2 Valida\u00e7\u00e3o com gestores DPO + Assessoria SPF 1 dia (por ciclo) Fase 3 Prepara\u00e7\u00e3o para piloto Assessoria SPF 1 dia Fase 3 Teste piloto com a \u00e1rea Assessoria SPF + DPO 5 dias Fase 3 Coleta e an\u00e1lise de feedbacks Assessoria SPF 1 dia Fase 3 Ajustes finais Assessoria SPF 3 dias Fase 3 Homologa\u00e7\u00e3o e entrega DPO + Assessoria SPF 1 dia <p>Tempo total estimado</p> <ul> <li>Fase inicial: 10 dias \u00fateis </li> <li>Desenvolvimento (3 ciclos por aba): 42 dias \u00fateis </li> <li>Fase final de testes: 10 dias \u00fateis Total: 62 dias \u00fateis</li> </ul>"},{"location":"dashboards/dpo/projeto/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>O cronograma pode sofrer ajustes conforme surgirem situa\u00e7\u00f5es imprevistas ou limita\u00e7\u00f5es t\u00e9cnicas. Por exemplo, nas fases iniciais, foi necess\u00e1rio utilizar Python e GitHub Actions para o tratamento automatizado dos dados devido ao alto volume e limita\u00e7\u00e3o do Power BI/hardware.</p> <p>Essa rotina Python automatizada ser\u00e1 documentada em uma se\u00e7\u00e3o pr\u00f3pria, dada sua complexidade e import\u00e2ncia para o funcionamento do painel.</p>"},{"location":"dashboards/guides/github/access_refresh_tokens/","title":"Access Token e Refresh Token","text":""},{"location":"dashboards/guides/github/access_refresh_tokens/#o-que-sao-access-token-e-refresh-token","title":"O que s\u00e3o Access Token e Refresh Token?","text":"<p>Segundo Auth0, quando um usu\u00e1rio faz login, o servidor de autoriza\u00e7\u00e3o emite um token de acesso (Access Token), que \u00e9 um artefato que as aplica\u00e7\u00f5es cliente podem usar para fazer chamadas seguras para um servidor de API. Quando uma aplica\u00e7\u00e3o cliente precisa acessar recursos protegidos em um servidor em nome de um usu\u00e1rio, o token de acesso permite que o cliente sinalize ao servidor que recebeu autoriza\u00e7\u00e3o do usu\u00e1rio para executar determinadas tarefas ou acessar determinados recursos. </p> <p>\u00c9 fundamental ter estrat\u00e9gias de seguran\u00e7a que minimizem o risco de comprometer os tokens de acesso. Um m\u00e9todo de mitiga\u00e7\u00e3o \u00e9 criar tokens de acesso com vida \u00fatil curta: eles s\u00e3o v\u00e1lidos apenas por um curto per\u00edodo definido em termos de horas ou dias. </p> <p>Depois que expiram, as aplica\u00e7\u00f5es cliente podem usar um Refresh Token para \"atualizar\" o token de acesso. Ou seja, um Refresh Token \u00e9 um artefato de credencial que permite que uma aplica\u00e7\u00e3o cliente obtenha novos tokens de acesso sem precisar solicitar que o usu\u00e1rio fa\u00e7a login novamente. </p> <p>Dessa forma, devido ao tempo curto de expira\u00e7\u00e3o do Access Token, em rotinas de automa\u00e7\u00e3o que requerem credenciamento de usu\u00e1rios, como \u00e9 o caso do processamento dos dados no painel de Execu\u00e7\u00e3o Or\u00e7ament\u00e1ria, o Refresh Token \u00e9 utilizado para gerar novos Access Tokens continuamente e validar a execu\u00e7\u00e3o da rotina junto \u00e0 API espec\u00edfica que est\u00e1 sendo utilizada.</p>"},{"location":"dashboards/guides/github/criar_secrets_pat/","title":"Setup Secrets, PAT e Actions Github","text":"<p>\u00c9 necess\u00e1rio criar o Secret e o Personal Access Token (PAT) no Github para correta execu\u00e7\u00e3o dos Actions do Github quando o reposit\u00f3rio for privado. </p>"},{"location":"dashboards/guides/github/criar_secrets_pat/#criar-pat","title":"Criar PAT","text":"<p>Conta &gt; Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic) &gt; Generate new token &gt; Generate new token (classic) &gt; Note: gh-action-push &gt; Select scopes: repo (marcar checkbox) &gt; Generate token &gt; Copiar o hash do token </p> <p>Sugest\u00e3o: copiar e salvar para um txt, pois voc\u00ea n\u00e3o conseguir\u00e1 ver ele novamente. </p> <p> </p>"},{"location":"dashboards/guides/github/criar_secrets_pat/#criar-secret","title":"Criar Secret","text":"<p>Reposit\u00f3rio &gt; Settings &gt; Secrets and variables &gt; Actions &gt; New repository secret &gt; Name: PERSONAL_TOKEN, Secret: colar a hash do PAT que voc\u00ea copiou &gt; Add secret </p> <p> </p>"},{"location":"dashboards/guides/github/github_actions/","title":"O que \u00e9 o Github Actions?","text":"<p>Segundo a documenta\u00e7\u00e3o oficial do Github, \u201co gitHub fornece m\u00e1quinas virtuais do Linux, Windows e macOS para executar seus fluxos de trabalho, ou voc\u00ea pode hospedar seus pr\u00f3prios executores auto-hospedados na sua pr\u00f3pria infraestrutura de dados ou na nuvem.\u201d </p> <p>A partir da configura\u00e7\u00e3o de um arquivo de script .yml \u00e9 poss\u00edvel estipular a execu\u00e7\u00e3o desses fluxos de trabalho pelas m\u00e1quinas virtuais disponibilizadas pelo github, de forma autom\u00e1tica, definindo inclusive agendamentos de hor\u00e1rios para execu\u00e7\u00e3o (chamados de cron jobs). </p> <p>A partir das VMs disponibilizadas pelo Github Actions e pela defini\u00e7\u00e3o dos cron jobs por meio do arquivo <code>.yml</code> se torna poss\u00edvel automa\u00e7\u00e3o de tarefas atrav\u00e9s da execu\u00e7\u00e3o autom\u00e1tica de scripts. </p>"},{"location":"dashboards/guides/pbi/carregar_dados/","title":"Carregar dados PBI","text":""},{"location":"dashboards/guides/pbi/carregar_dados/#carregamento-de-dados-de-planilha-do-sharepoint-no-power-bi","title":"Carregamento de dados de Planilha do Sharepoint no Power BI","text":"<ul> <li> <p>Obter dados no Power BI  </p> </li> <li> <p>Extrair links da planilha no Sharepoint  </p> </li> <li> <p>Colar link no Power BI </p> </li> </ul>"},{"location":"dashboards/guides/pbi/carregar_dados/#carregamento-de-dados-do-github-no-power-bi","title":"Carregamento de dados do Github no Power BI","text":"<ul> <li> <p>Extrair links das bases  </p> </li> <li> <p>Criar par\u00e2metro no PBI e copiar e colar a hash do Personal Access Token (PAT) do Github, criado em momento pr\u00e9vio </p> </li> <li> <p>Criar consulta nula que ir\u00e1 armazenar os dados</p> </li> </ul> <p></p> <ul> <li>Carregar dados via Power Query M, copiando e colando o c\u00f3digo abaixo no PBI, alterando a URL</li> </ul> <p></p> <p>O c\u00f3digo citado serve para consumir os dados via API do pr\u00f3prio github, sendo passado no cabe\u00e7alho da requisi\u00e7\u00e3o o tipo Bearer e o PAT.</p> <pre><code>url = \"https://api.github.com/repos/SPF-SEPLAG/painel-dpo/contents/processed_data_num.csv\",\nauthHeader = \"Bearer \" &amp; GitHubToken,\nresponse = Json.Document(Web.Contents(url, [\n    Headers = [Authorization = authHeader]\n])),\nbase64 = response[content],\nbinary = Binary.FromText(base64, BinaryEncoding.Base64),\ncsv = Csv.Document(binary, [Delimiter = \",\", Encoding = 65001, QuoteStyle = QuoteStyle.Csv]),\ntable = Table.PromoteHeaders(csv),\n</code></pre>"},{"location":"dashboards/guides/pbi/configurar_modelo_semantico/","title":"Configurar Modelo Sem\u00e2ntico PBI","text":"<ul> <li>Uma vez elaborado o relat\u00f3rio, ser\u00e1 necess\u00e1rio realizar a publica\u00e7\u00e3o dos dados </li> </ul>"},{"location":"dashboards/guides/pbi/configurar_modelo_semantico/#gerar-o-link-do-dashboard","title":"Gerar o link do dashboard","text":"<ul> <li>Acesse o relat\u00f3rio do painel no PBI Web </li> </ul>"},{"location":"dashboards/guides/pbi/configurar_modelo_semantico/#acessar-o-modelo-semantico","title":"Acessar o modelo sem\u00e2ntico","text":""},{"location":"dashboards/guides/pbi/configurar_modelo_semantico/#configurar-credenciais-bases-github","title":"Configurar credenciais bases Github","text":"<ul> <li>Definir as credenciais de acesso para as bases do github  </li> </ul>"},{"location":"dashboards/guides/pbi/configurar_modelo_semantico/#configurar-credenciais-bases-excel-sharepoint","title":"Configurar credenciais bases Excel Sharepoint","text":""},{"location":"dashboards/guides/pbi/configurar_modelo_semantico/#definir-periodicidade-de-atualizacao-dos-paineis","title":"Definir periodicidade de atualiza\u00e7\u00e3o dos pain\u00e9is","text":"<ul> <li>\u00c9 poss\u00edvel definir a frequ\u00eancia de atualiza\u00e7\u00e3o e inserir contato de e-mail que ser\u00e1 notificado em caso de erro de atualiza\u00e7\u00e3o  </li> </ul> <p>Tamb\u00e9m \u00e9 poss\u00edvel realizar a atualiza\u00e7\u00e3o manualmente caso necess\u00e1rio  </p>"},{"location":"dashboards/guides/sap/rotina_bo/","title":"Rotina bo","text":"<p>Documentar - fluxos automa\u00e7\u00f5es</p> <ul> <li>Este documento tem o intuito de documentar a rotina de atualiza\u00e7\u00e3o de dados de painel BI do BO institucional via Power Automate. </li> <li>Introdu\u00e7\u00e3o sobre Painel DPO</li> <li>Processo de Tratamento de dados Python Painel DPO  </li> <li> <p>Elaborar p\u00e1gina introdut\u00f3ria na se\u00e7\u00e3o overview dashboards direcionando os links</p> </li> <li> <p>Explicar l\u00f3gica de constru\u00e7\u00e3o dos pain\u00e9is</p> </li> <li> <p>Salvar arquivos dos paineis</p> </li> <li>CLonar repos github</li> <li>Salvar arquivos das automa\u00e7\u00f5es</li> </ul>"}]}